{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "46d372b3",
      "metadata": {
        "id": "46d372b3"
      },
      "source": [
        "# Linear Regression & Gradient Descent\n",
        "\n",
        "_In this notebook we will:_\n",
        "1. Review the model, loss & cost functions for simple linear regression.  \n",
        "2. Implement **Gradient Descent** from scratch and visualize how the line converges.  \n",
        "3. Experiment with learning rate and see oscillations or divergence.  \n",
        "4. Apply to a small **day-ahead market** price forecast using demand as feature.  \n",
        "5. Compare our implementation to `sklearn.linear_model.LinearRegression`.  \n",
        "6. Introduce **MAE** and **RMSE** and compare single vs. multiple regression.\n",
        "7. Practice with multiple linear regression using additional features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6ce2eaf",
      "metadata": {
        "id": "d6ce2eaf"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Model, Loss & Cost Review\n",
        "\n",
        "We assume a model with a single feature $x$:\n",
        "\n",
        "$$\n",
        "\\hat y^{(i)} = \\beta_0 + \\beta_1\\,x^{(i)}\n",
        "$$\n",
        "\n",
        "The **loss** for one sample is the squared error:\n",
        "\n",
        "$$\n",
        "L^{(i)} = \\bigl(y^{(i)} - \\hat y^{(i)}\\bigr)^2\n",
        "               = \\bigl(y^{(i)} - (\\beta_0 + \\beta_1\\,x^{(i)})\\bigr)^2\n",
        "$$\n",
        "\n",
        "The **cost** (MSE) over $m$ samples is:\n",
        "\n",
        "$$\n",
        "J(\\beta_0,\\beta_1)\n",
        "= \\frac{1}{m}\\sum_{i=1}^m \\bigl(y^{(i)} - (\\beta_0 + \\beta_1\\,x^{(i)})\\bigr)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b5bf29a",
      "metadata": {
        "id": "3b5bf29a"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Gradient Descent Derivation\n",
        "\n",
        "We update each parameter by moving **against** the gradient of $J$:\n",
        "\n",
        "$$\n",
        "\\beta_j \\;\\leftarrow\\; \\beta_j\n",
        "                - \\alpha \\;\\frac{\\partial J}{\\partial \\beta_j}\n",
        "\\,,\\quad j=0,1\n",
        "$$\n",
        "\n",
        "Compute partial derivatives:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\beta_0}\n",
        "= -\\frac{2}{m}\\sum_{i=1}^m \\bigl(y^{(i)} - (\\beta_0 + \\beta_1 x^{(i)})\\bigr)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\beta_1}\n",
        "= -\\frac{2}{m}\\sum_{i=1}^m \\bigl(y^{(i)} - (\\beta_0 + \\beta_1 x^{(i)})\\bigr)\\,x^{(i)}\n",
        "$$\n",
        "\n",
        "Thus the updates become:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\beta_0 &\\leftarrow \\beta_0\n",
        "              + \\frac{2\\alpha}{m}\\sum_{i=1}^m \\bigl(y^{(i)} - (\\beta_0 + \\beta_1 x^{(i)})\\bigr),\\\\\n",
        "\\beta_1 &\\leftarrow \\beta_1\n",
        "              + \\frac{2\\alpha}{m}\\sum_{i=1}^m \\bigl(y^{(i)} - (\\beta_0 + \\beta_1 x^{(i)})\\bigr)\\,x^{(i)}.\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6711c6b7",
      "metadata": {
        "id": "6711c6b7"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Implementing Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32170527",
      "metadata": {
        "id": "32170527"
      },
      "source": [
        "First, we import all the libraries we will use in this notebook. These include:\n",
        "- `numpy` for numerical operations\n",
        "- `pandas` for data manipulation\n",
        "- `matplotlib` for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff46924",
      "metadata": {
        "id": "3ff46924"
      },
      "outputs": [],
      "source": [
        "import numpy as np # for numerical operations\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import pandas as pd # for data manipulation\n",
        "\n",
        "# import also ipython widgets for interactive plotting\n",
        "from ipywidgets import interact, FloatLogSlider\n",
        "\n",
        "import os # to handle file paths\n",
        "import urllib.request # to download files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4116b0d",
      "metadata": {
        "id": "e4116b0d"
      },
      "outputs": [],
      "source": [
        "# 3.1 Generate synthetic data\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate temperature data (feature) between -10 and 35 degrees\n",
        "temperature = np.random.uniform(-10, 35, size=50)\n",
        "\n",
        "# Generate energy demand (target) with relationship: demand = 250 - 3*temperature + noise\n",
        "# This simulates the inverse relationship between temperature and energy demand\n",
        "energy_demand = 250 - 3*temperature + np.random.normal(0, 10, size=temperature.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d9fb67",
      "metadata": {
        "id": "95d9fb67"
      },
      "outputs": [],
      "source": [
        "# 3.2 Visualize the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(temperature, energy_demand, color='blue', alpha=0.7)\n",
        "plt.title('Energy Demand vs Temperature', fontsize=14)\n",
        "plt.xlabel('Temperature (°C)', fontsize=12)\n",
        "plt.ylabel('Energy Demand (MW)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Print a brief description of the data\n",
        "print(f\"Data summary:\")\n",
        "print(f\"Number of samples: {len(temperature)}\")\n",
        "print(f\"Temperature range: {temperature.min():.1f}°C to {temperature.max():.1f}°C\")\n",
        "print(f\"Energy demand range: {energy_demand.min():.1f} MW to {energy_demand.max():.1f} MW\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0ac9fc",
      "metadata": {
        "id": "ed0ac9fc"
      },
      "outputs": [],
      "source": [
        "# 3.3 Define prediction function\n",
        "def predict(X, intercept, slope):\n",
        "    \"\"\"Calculate predicted values using linear model.\n",
        "\n",
        "    Args:\n",
        "        X: Feature values (temperature)\n",
        "        intercept: The y-intercept (beta0)\n",
        "        slope: The slope coefficient (beta1)\n",
        "\n",
        "    Returns:\n",
        "        Predicted values based on linear equation: intercept + slope * X\n",
        "    \"\"\"\n",
        "    return intercept + slope * X\n",
        "\n",
        "# 3.4 Compute Mean Squared Error (MSE) cost function\n",
        "def compute_cost(X, y_true, intercept, slope):\n",
        "    \"\"\"Calculate the Mean Squared Error cost.\n",
        "\n",
        "    Args:\n",
        "        X: Feature values (temperature)\n",
        "        y_true: Actual target values (energy demand)\n",
        "        intercept: The y-intercept (beta0)\n",
        "        slope: The slope coefficient (beta1)\n",
        "\n",
        "    Returns:\n",
        "        Mean Squared Error cost value\n",
        "    \"\"\"\n",
        "    m = len(y_true)  # Number of samples\n",
        "    y_pred = predict(X, intercept, slope)  # Get predictions\n",
        "    squared_errors = (y_true - y_pred)**2  # Calculate squared errors\n",
        "    return np.sum(squared_errors) / m  # Return mean squared error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a-0O6-dbQ0Hd",
      "metadata": {
        "id": "a-0O6-dbQ0Hd"
      },
      "source": [
        "### Exercise 1: Implement Gradient Descent for Linear Regression\n",
        "\n",
        "In this exercise, you will fill in the core functions that power gradient descent and then test your implementation on the synthetic temperature–demand dataset.\n",
        "\n",
        "**Your tasks:**\n",
        "\n",
        "1. **Complete** the `compute_gradients(X, y_true, intercept, slope)` function:\n",
        "   - Compute the vector of predictions `y_pred = intercept + slope * X`.\n",
        "   - Calculate the error `error = y_true - y_pred`.\n",
        "   - Return the two partial derivatives as described above\n",
        "\n",
        "2. **Complete** the `gradient_descent(X, y_true, learning_rate, num_iterations)` function:\n",
        "   - Initialize `intercept = 0.0` and `slope = 0.0`.\n",
        "   - For each iteration:\n",
        "     1. Call your `compute_gradients(...)` to get `grad_intercept` and `grad_slope`.\n",
        "     2. Update the parameters:  \n",
        "        ```python\n",
        "        intercept -= learning_rate * grad_intercept\n",
        "        slope     -= learning_rate * grad_slope\n",
        "        ```\n",
        "     3. Store the current `intercept`, `slope`, and `compute_cost(...)` in a history dictionary for later plotting.\n",
        "\n",
        "3. **Run** your gradient descent on the provided synthetic data:\n",
        "   ```python\n",
        "   learning_rate = 0.002\n",
        "   iterations    = 5000\n",
        "\n",
        "   intercept_gd, slope_gd, history = gradient_descent(temperature, energy_demand,\n",
        "                                                      learning_rate, iterations)\n",
        "\n",
        "4. **Visualize**:\n",
        "\n",
        "   * Plot the cost (MSE) vs. iteration number to verify that it’s decreasing smoothly.\n",
        "   * Overlay the initial line (iteration 0) and your final fitted line on the scatter of `(temperature, energy_demand)`.\n",
        "\n",
        "---\n",
        "\n",
        "**Hints:**\n",
        "\n",
        "* Use the formulas from Section 2 for the partial derivatives.\n",
        "* Make sure your history dictionary keys are exactly `['intercept', 'slope', 'cost']`.\n",
        "* If the cost increases or oscillates, try reducing `learning_rate` by an order of magnitude.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a064b9e",
      "metadata": {
        "id": "8a064b9e"
      },
      "outputs": [],
      "source": [
        "# 3.5 Compute gradients - YOUR CODE HERE\n",
        "def compute_gradients(X, y_true, intercept, slope):\n",
        "    \"\"\"Compute the gradients for intercept and slope parameters.\n",
        "\n",
        "    Args:\n",
        "        X: Feature values (temperature)\n",
        "        y_true: Actual target values (energy demand)\n",
        "        intercept: Current intercept parameter value\n",
        "        slope: Current slope parameter value\n",
        "\n",
        "    Returns:\n",
        "        grad_intercept: Gradient for intercept parameter\n",
        "        grad_slope: Gradient for slope parameter\n",
        "    \"\"\"\n",
        "    # TODO: Implement the gradient computation\n",
        "    # Hint: Use the formulas from the derivation\n",
        "    m = len(y_true)  # Number of samples\n",
        "\n",
        "    # Your code here\n",
        "    # 1. Calculate predictions using current parameters\n",
        "    # 2. Calculate the errors (y_true - predictions)\n",
        "    # 3. Calculate gradients using the formulas provided earlier\n",
        "    grad_intercept = None\n",
        "    grad_slope = None\n",
        "\n",
        "    return grad_intercept, grad_slope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a736040c",
      "metadata": {
        "id": "a736040c"
      },
      "outputs": [],
      "source": [
        "# 3.6 Gradient descent loop with history - YOUR CODE HERE\n",
        "def gradient_descent(X, y_true, learning_rate, num_iterations):\n",
        "    \"\"\"Perform gradient descent to find optimal intercept and slope.\n",
        "\n",
        "    Args:\n",
        "        X: Feature values (temperature)\n",
        "        y_true: Actual target values (energy demand)\n",
        "        learning_rate: Step size for parameter updates (alpha)\n",
        "        num_iterations: Number of iterations to run\n",
        "\n",
        "    Returns:\n",
        "        intercept: Final intercept value (beta0)\n",
        "        slope: Final slope value (beta1)\n",
        "        history: Dictionary with parameter and cost history for visualization\n",
        "    \"\"\"\n",
        "    # Initialize parameters to zero\n",
        "    intercept, slope = 0.0, 0.0\n",
        "\n",
        "    # Initialize history dictionary to track parameters and cost\n",
        "    history = {'intercept':[], 'slope':[], 'cost':[]}\n",
        "\n",
        "    # TODO: Implement the gradient descent algorithm\n",
        "    # Your code here\n",
        "    # 1. Loop through specified number of iterations\n",
        "    # 2. Compute gradients using compute_gradients function\n",
        "    # 3. Update parameters using learning rate and gradients\n",
        "    # 4. Store parameters and cost in history dictionary\n",
        "\n",
        "    return intercept, slope, history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c558b4d",
      "metadata": {},
      "source": [
        "Run the following cell to test your implementation. It will generate a synthetic dataset and run your gradient descent implementation. If everything is correct, you will se a massege indicating that all checks passed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41bf7a83",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_compute_gradients_and_gd():\n",
        "    X_test = np.array([0, 1, 2, 3, 4])\n",
        "    y_test = np.array([1, 3, 5, 7, 9])  # True model: y = 2x + 1\n",
        "\n",
        "    # Check gradients at intercept=0.0, slope=0.0\n",
        "    grad_intercept, grad_slope = compute_gradients(X_test, y_test, intercept=0.0, slope=0.0)\n",
        "    assert np.isclose(grad_intercept, -10.0), f\"Expected grad_intercept -10.0, got {grad_intercept}\"\n",
        "    assert np.isclose(grad_slope, -28.0), f\"Expected grad_slope -28.0, got {grad_slope}\"\n",
        "\n",
        "    # Check convergence of gradient descent\n",
        "    intercept, slope, history = gradient_descent(X_test, y_test, learning_rate=0.01, num_iterations=1000)\n",
        "    assert np.isclose(intercept, 1.0, atol=0.1), f\"Expected intercept ≈ 1.0, got {intercept}\"\n",
        "    assert np.isclose(slope, 2.0, atol=0.1), f\"Expected slope ≈ 2.0, got {slope}\"\n",
        "\n",
        "    print(\"✅ All gradient checks passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_compute_gradients_and_gd()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88623641",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Visualizing Convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e543d99",
      "metadata": {},
      "source": [
        "### Experiment: Tuning the Learning Rate with a Slider\n",
        "\n",
        "Gradient descent performance depends heavily on the choice of learning rate (`α`). If it's too small, convergence is slow; if it's too large, the algorithm may diverge or oscillate.\n",
        "\n",
        "In the interactive plot below, you can adjust the learning rate using a log-scale slider and observe how it affects:\n",
        "\n",
        "- The **speed and stability** of convergence,\n",
        "- The **final cost value** after training,\n",
        "- Whether the model converges at all.\n",
        "\n",
        "Use this to build intuition for how sensitive gradient-based optimization is to hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb472a15",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_and_plot(learning_rate=0.002):\n",
        "    iterations = 10000\n",
        "    \n",
        "    intercept, slope, history = gradient_descent(temperature, energy_demand, learning_rate, iterations)\n",
        "    \n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(iterations), history['cost'], color='blue')\n",
        "    plt.title(f'Cost Convergence (learning_rate={learning_rate})', fontsize=14)\n",
        "    plt.xlabel('Iteration', fontsize=12)\n",
        "    plt.ylabel('Cost (MSE)', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Intercept (β₀): {intercept:.2f}\")\n",
        "    print(f\"Slope (β₁): {slope:.2f}\")\n",
        "    print(f\"Final Cost: {history['cost'][-1]:.2f}\")\n",
        "\n",
        "# Use a log-scale slider for better control\n",
        "interact(run_and_plot, learning_rate=FloatLogSlider(value=0.002, base=10, min=-5, max=-1, step=0.1, description='Learning rate'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4913a3d3",
      "metadata": {},
      "source": [
        "Now when you found a good learning rate, we can run the gradient descent algorithm on the synthetic dataset and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debf1bda",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run gradient descent and plot cost over iterations\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.002  # Step size for parameter updates\n",
        "iterations = 10000    # Number of iterations to run\n",
        "\n",
        "# Run gradient descent algorithm\n",
        "intercept, slope, history = gradient_descent(temperature, energy_demand, learning_rate, iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c331d64c",
      "metadata": {
        "id": "c331d64c"
      },
      "outputs": [],
      "source": [
        "# Visualize cost convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(iterations), history['cost'], color='blue')\n",
        "plt.title('Cost Function Convergence Over Iterations', fontsize=14)\n",
        "plt.xlabel('Iteration Number', fontsize=12)\n",
        "plt.ylabel('Cost (MSE)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add annotation showing final cost\n",
        "plt.annotate(f'Final cost: {history[\"cost\"][-1]:.1f}',\n",
        "             xy=(0.7, 0.9), xycoords='axes fraction',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7f446c",
      "metadata": {
        "id": "5e7f446c"
      },
      "outputs": [],
      "source": [
        "# 4.2 Visualize model convergence by plotting intermediate lines\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot original data points\n",
        "plt.scatter(temperature, energy_demand, s=30, color='blue', label='Data Points')\n",
        "\n",
        "# Plot intermediate lines (every 100 iterations)\n",
        "step_size = 100  # Plot line every 100 iterations for visibility\n",
        "for i in range(0, iterations, step_size):\n",
        "    intercept_i, slope_i = history['intercept'][i], history['slope'][i]\n",
        "\n",
        "    # Create x-values spanning the temperature range\n",
        "    x_range = np.array([temperature.min(), temperature.max()])\n",
        "\n",
        "    # Plot the intermediate line with low opacity\n",
        "    if i == 0:\n",
        "        plt.plot(x_range, predict(x_range, intercept_i, slope_i),\n",
        "                 color='green', alpha=0.5, label='Initial line')\n",
        "    else:\n",
        "        plt.plot(x_range, predict(x_range, intercept_i, slope_i),\n",
        "                 color='gray', alpha=0.4)\n",
        "\n",
        "# Plot final regression line\n",
        "plt.plot(x_range, predict(x_range, intercept, slope),\n",
        "         color='red', linewidth=2, label='Final Regression Line')\n",
        "\n",
        "# Add equation of the line as text\n",
        "equation = f'y = {intercept:.1f} {slope:.1f}x'\n",
        "plt.annotate(equation, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
        "\n",
        "plt.title('Gradient Descent Progress Visualization', fontsize=14)\n",
        "plt.xlabel('Temperature (°C)', fontsize=12)\n",
        "plt.ylabel('Energy Demand (MW)', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Print the final parameters and cost\n",
        "print(f\"Results after {iterations} iterations:\")\n",
        "print(f\"Intercept (β₀): {intercept:.2f}\")\n",
        "print(f\"Slope (β₁): {slope:.2f}\")\n",
        "print(f\"Cost value: {history['cost'][-1]:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e176224",
      "metadata": {},
      "source": [
        "### Check Your Results — Gradient Descent on Synthetic Dataset\n",
        "\n",
        "If you've implemented `gradient_descent()` correctly and used the provided synthetic temperature–demand dataset, your results should be **close to the following**:\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "\n",
        "Results after 10000 iterations using learning rate 0.002:\n",
        "Intercept (β₀): 249.25\n",
        "Slope (β₁): -3.07\n",
        "Cost value: 85.6\n",
        "\n",
        "```\n",
        "\n",
        "This corresponds to the underlying true relationship:\n",
        "\n",
        "> **energy_demand ≈ 250 - 3 × temperature**\n",
        "\n",
        "Your **cost should decrease smoothly** over iterations, and the final **regression line** should fit the data well.\n",
        "\n",
        "**If your results are far off:**\n",
        "- Check that you're using the **correct gradient formulas** (especially the signs!).\n",
        "- Try reducing the **learning rate** if the cost oscillates or increases.\n",
        "- Print intermediate values to debug the update loop.\n",
        "\n",
        "---\n",
        "\n",
        "You’re on the right track if your values are **close** to the expected ones. Don’t worry if they’re not exact — convergence may vary slightly depending on learning rate and numerical precision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b2d41dd",
      "metadata": {
        "id": "6b2d41dd"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Day-Ahead Market Example\n",
        "\n",
        "In this section, we'll apply our gradient descent implementation to a real-world energy market dataset for Germany from the year 2024. First, let's understand how to load the data.\n",
        "\n",
        "**About the Dataset**\n",
        "\n",
        "The day-ahead electricity market data used in this exercise is sourced from the **[SMARD.de](https://www.smard.de/home)** platform — the official transparency portal of the German electricity market operated by the Bundesnetzagentur.\n",
        "\n",
        "SMARD provides free access to a wide range of real-time and historical data including:\n",
        "\n",
        "- Electricity generation by source,\n",
        "- Day-ahead and intraday market prices,\n",
        "- Load forecasts and actual consumption,\n",
        "- Cross-border electricity flows.\n",
        "\n",
        "You are encouraged to explore this portal for additional datasets, either for your own projects or to extend this analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HCSyZfG1OrYZ",
      "metadata": {
        "id": "HCSyZfG1OrYZ"
      },
      "source": [
        "### 5.1 Loading Data with Pandas\n",
        "\n",
        "Pandas is a data manipulation library in Python. Here's how to load CSV files using pandas:\n",
        "\n",
        "- `pd.read_csv()`: Loads data from a CSV file into a pandas DataFrame\n",
        "- `parse_dates`: Specifies which columns should be parsed as datetime objects\n",
        "- `index_col`: Specifies which column should be used as the DataFrame's index\n",
        "\n",
        "In our case, the first column contains the date and time, and we want to parse it as a datetime object."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bca02ad",
      "metadata": {},
      "source": [
        "**This cell will create an `inputs/` folder (if it doesn't already exist) and then download the `market_data_2024.csv` file from the GitHub repository into that folder.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8626aefb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Create the 'inputs' folder if it doesn't exist\n",
        "os.makedirs('inputs', exist_ok=True)\n",
        "\n",
        "# 2) Define the URL to the raw CSV on GitHub and the local path\n",
        "url = 'https://raw.githubusercontent.com/nick-harder/AIES/main/lecture7/data/market_data_2024.csv'\n",
        "local_path = 'inputs/market_data_2024.csv'\n",
        "\n",
        "# 3) Download the file only if it's not already present\n",
        "if not os.path.exists(local_path):\n",
        "    print(f\"Downloading data from {url} ...\")\n",
        "    urllib.request.urlretrieve(url, local_path)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(\"File already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcc2c82d",
      "metadata": {
        "id": "dcc2c82d"
      },
      "outputs": [],
      "source": [
        "# Load the day-ahead market data\n",
        "# - index_col='date': Sets the 'date' column as the index of the DataFrame\n",
        "# - parse_dates=True: Converts date strings to datetime objects\n",
        "df_full = pd.read_csv('inputs/market_data_2024.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "# Display the first few rows to understand the data structure\n",
        "print(\"Full dataset preview:\")\n",
        "display(df_full.head())\n",
        "\n",
        "# Select only one month (June 2024) for simplicity\n",
        "df = df_full.loc['2024-06'].copy()\n",
        "\n",
        "print(f\"\\nSelected month (June 2024) data shape: {df.shape}\")\n",
        "display(df.head())\n",
        "\n",
        "# Extract features and target\n",
        "demand = df['Demand'].values  # Feature (X)\n",
        "price = df['DA Price'].values    # Target (Y)\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"\\nData statistics:\")\n",
        "print(f\"Number of data points: {len(demand)}\")\n",
        "print(f\"Demand range: {demand.min():.1f} to {demand.max():.1f} MW\")\n",
        "print(f\"Price range: {price.min():.2f} to {price.max():.2f} €/MWh\")\n",
        "\n",
        "# Visualize the relationship between demand and price\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(demand, price, alpha=0.7)\n",
        "plt.title('Day-Ahead Price vs. Demand (June 2024)', fontsize=14)\n",
        "plt.xlabel('Demand (MW)', fontsize=12)\n",
        "plt.ylabel('Price (€/MWh)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f842c789",
      "metadata": {
        "id": "f842c789"
      },
      "source": [
        "### 5.2 Applying Gradient Descent to Raw Energy Market Data\n",
        "\n",
        "In this section, we apply our gradient descent implementation to a real-world energy market dataset. We will first use raw data to observe the performance of our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179aa7b3",
      "metadata": {
        "id": "179aa7b3"
      },
      "source": [
        "#### Exercise 2: Run Gradient Descent on Raw Data\n",
        "\n",
        "In the cell below, run your `gradient_descent` on the **raw** `demand` and `price` arrays **without** any normalization.  \n",
        "Observe what happens to the cost curve—does it converge, oscillate, or diverge?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57aacf6",
      "metadata": {},
      "source": [
        "**Note**: If the cost increases or becomes `NaN`, this indicates that the learning rate is too high. This is excpected at this point in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeebd328",
      "metadata": {
        "id": "aeebd328"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters (you may need to tweak these to try to force convergence)\n",
        "learning_rate = 0.002\n",
        "iterations = 10000\n",
        "\n",
        "# --- YOUR CODE HERE ---\n",
        "# Run gradient descent on raw (unnormalized) data\n",
        "intercept_raw, slope_raw, history_raw = gradient_descent(demand, price, learning_rate, iterations)\n",
        "\n",
        "# Plot cost over iterations\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history_raw['cost'])\n",
        "plt.title('Cost vs Iterations (Raw Data)')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final cost: {history_raw['cost'][-1]:.2e}\")\n",
        "print(f\"Intercept: {intercept_raw:.4f}, Slope: {slope_raw:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d8cdb5",
      "metadata": {
        "id": "a0d8cdb5"
      },
      "source": [
        "\n",
        "> **Questions for Exercise 2:**  \n",
        "> 1. Does the cost decrease smoothly?  \n",
        "> 2. If not, why might gradient descent be failing here?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec8d28a4",
      "metadata": {},
      "source": [
        "### 5.3 Applying Gradient Descent to Normalized Data\n",
        "\n",
        "As we saw in the previous section, using the raw data resulted in a failure of our gradient descent implementation. The algorithm either diverged or failed to converge due to unstable updates.\n",
        "\n",
        "#### Why Did This Happen?\n",
        "\n",
        "Gradient descent updates each parameter based on the slope (gradient) of the cost function. For the slope parameter $\\beta_1$, the update rule is:\n",
        "\n",
        "$$\n",
        "\\beta_1 \\leftarrow \\beta_1 - \\alpha \\left( -\\frac{2}{m} \\sum_{i=1}^m \\left(y^{(i)} - (\\beta_0 + \\beta_1 x^{(i)})\\right) \\cdot x^{(i)} \\right)\n",
        "$$\n",
        "\n",
        "If the feature values $x$ are large in magnitude (e.g., demand measured in thousands of MW), the gradients become large as well. This requires using a very small learning rate $\\alpha$ to prevent the updates from overshooting the minimum.\n",
        "\n",
        "Without normalization, the optimizer may:\n",
        "- Overshoot or diverge,\n",
        "- Zigzag inefficiently,\n",
        "- Converge very slowly or not at all.\n",
        "\n",
        "\n",
        "> What you see on the following plot is the cost surface resulting from applying linear regression to raw (unnormalized) demand and price data. The surface appears extremely stretched and narrow, especially along the slope $\\beta_1$ axis—this shape illustrates why gradient descent struggles in this setting.\n",
        ">\n",
        "> This instability is caused by the **unequal scaling** between input features and target values, which distorts the shape of the optimization landscape.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01875e2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing Cost Surface on Raw Price-Demand Data\n",
        "\n",
        "# Use June 2024 data loaded earlier\n",
        "X = demand  # Feature\n",
        "y = price   # Target\n",
        "\n",
        "# Define parameter ranges for intercept and slope\n",
        "b0_range = np.linspace(-300, 300, 100)\n",
        "b1_range = np.linspace(-2, 2, 100)\n",
        "\n",
        "# Mesh grid for parameter space\n",
        "B0, B1 = np.meshgrid(b0_range, b1_range)\n",
        "cost_surface = np.zeros_like(B0)\n",
        "\n",
        "# Compute MSE for each (β₀, β₁) combination\n",
        "for i in range(B0.shape[0]):\n",
        "    for j in range(B0.shape[1]):\n",
        "        y_pred = B0[i, j] + B1[i, j] * X\n",
        "        cost_surface[i, j] = np.mean((y - y_pred) ** 2)\n",
        "\n",
        "# Plot the cost surface as a contour plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "cp = plt.contourf(B0, B1, cost_surface, levels=50, cmap='viridis')\n",
        "plt.colorbar(cp)\n",
        "plt.title(\"Cost Surface on Raw Price vs Demand Data\", fontsize=14)\n",
        "plt.xlabel(\"Intercept (β₀)\", fontsize=12)\n",
        "plt.ylabel(\"Slope (β₁)\", fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# Inspect data scale\n",
        "print(f\"Demand range: {X.min():.1f} MW to {X.max():.1f} MW\")\n",
        "print(f\"Price range: {y.min():.2f} €/MWh to {y.max():.2f} €/MWh\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d1a240",
      "metadata": {},
      "source": [
        "#### Z-score Normalization\n",
        "\n",
        "To address this, we apply **z-score normalization** to the input and target data.\n",
        "\n",
        "Given a feature vector $x = [x^{(1)}, x^{(2)}, \\dots, x^{(n)}]$, the **z-score normalized** version $x'$ is defined as:\n",
        "\n",
        "$$\n",
        "x'^{(i)} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}\n",
        "\\quad \\text{for each } i = 1, \\dots, n\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\mu_x$ is the **mean** of the feature:\n",
        "  $$\n",
        "  \\mu_x = \\frac{1}{n} \\sum_{i=1}^n x^{(i)}\n",
        "  $$\n",
        "\n",
        "- $\\sigma_x$ is the **standard deviation**:\n",
        "  $$\n",
        "  \\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - \\mu_x)^2}\n",
        "  $$\n",
        "\n",
        "This method rescales all features to have mean 0 and standard deviation 1, transforming the optimization surface into a more balanced (spherical) shape. As a result:\n",
        "\n",
        "- Gradient descent converges faster  \n",
        "- Parameter updates remain stable  \n",
        "- The risk of exploding gradients is significantly reduced  \n",
        "\n",
        "Normalized inputs are also beneficial in many other machine learning models, such as k-NN, SVMs, and neural networks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55a7036",
      "metadata": {},
      "source": [
        "#### Exercise 3: Implement Z-Score Normalization\n",
        "\n",
        "Now apply normalization in practice:\n",
        "\n",
        "1. Compute the means and standard deviations for both `demand` and `price`.\n",
        "2. Normalize the data using the z-score formula.\n",
        "3. Run gradient descent on the **normalized** data.\n",
        "4. Denormalize the resulting model parameters to interpret them in the original units.\n",
        "5. Use the fitted model to compute predicted prices and visualize your results.\n",
        "\n",
        "Make sure to compare the convergence behavior and forecast quality with your previous run on unnormalized data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b462ec0",
      "metadata": {
        "id": "3b462ec0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate mean and standard deviation\n",
        "# TODO: Compute the mean and standard deviation for demand\n",
        "demand_mean = ...\n",
        "demand_std = ...\n",
        "\n",
        "# TODO: Compute the mean and standard deviation for price\n",
        "price_mean = ...\n",
        "price_std = ...\n",
        "\n",
        "# Step 2: Normalize demand and price\n",
        "# TODO: Normalize the demand and price arrays\n",
        "demand_normalized = ...\n",
        "price_normalized = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32de6de1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the cost surface again using normalized data\n",
        "X_norm = demand_normalized\n",
        "y_norm = price_normalized\n",
        "\n",
        "b0_range = np.linspace(-2, 2, 100)\n",
        "b1_range = np.linspace(-2, 2, 100)\n",
        "B0, B1 = np.meshgrid(b0_range, b1_range)\n",
        "cost_surface = np.zeros_like(B0)\n",
        "\n",
        "# Step 3: Compute MSE across the parameter grid\n",
        "for i in range(B0.shape[0]):\n",
        "    for j in range(B0.shape[1]):\n",
        "        y_pred = B0[i, j] + B1[i, j] * X_norm\n",
        "        cost_surface[i, j] = np.mean((y_norm - y_pred) ** 2)\n",
        "\n",
        "# Step 4: Plot the cost surface\n",
        "plt.figure(figsize=(8, 6))\n",
        "cp = plt.contourf(B0, B1, cost_surface, levels=50, cmap='viridis')\n",
        "plt.colorbar(cp)\n",
        "plt.title(\"Cost Surface After Z-Score Normalization\", fontsize=14)\n",
        "plt.xlabel(\"Intercept (β₀)\", fontsize=12)\n",
        "plt.ylabel(\"Slope (β₁)\", fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43b1a3a",
      "metadata": {},
      "source": [
        "What you see on this plot is the cost surface after applying **Z-score normalization** to both the demand and price data.\n",
        "\n",
        "In contrast to the stretched and narrow surface from the raw data, this plot shows a **symmetrical, bowl-shaped landscape**. The contours are nearly circular, indicating that the cost function changes at similar rates along both parameter axes (intercept and slope).\n",
        "\n",
        "This balanced geometry confirms that **normalization has improved the conditioning of the optimization problem**. Gradient descent now takes consistent, stable steps toward the minimum and is far less likely to overshoot or diverge.\n",
        "\n",
        "In short: this is the kind of landscape gradient descent is designed to perform well on — and why normalization is critical when working with real-world data that spans different scales.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66680bab",
      "metadata": {},
      "source": [
        "Now that we have normalized our data, we can run gradient descent on the normalized demand and price data. The cost function should converge smoothly, and the fitted line should closely match the underlying relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3693532",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Run gradient descent\n",
        "intercept, slope, history = gradient_descent(\n",
        "    demand_normalized, price_normalized, learning_rate, iterations\n",
        ")\n",
        "\n",
        "# Step 3: Denormalize the model parameters to original scale\n",
        "# If y = a*x + b in normalized space,\n",
        "# Then in original space: price = a*(demand - mean_d)/std_d * std_p + b*std_p + mean_p\n",
        "slope = slope * (price_std / demand_std)\n",
        "intercept = price_std * intercept + price_mean - slope * demand_mean\n",
        "\n",
        "# Step 4: calculate our predicted price\n",
        "our_predicted_price = predicted_price = intercept + slope * demand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ba68ae",
      "metadata": {
        "id": "d0ba68ae"
      },
      "outputs": [],
      "source": [
        "# Plot the convergence of the cost function\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(iterations), history['cost'], color='blue')\n",
        "plt.title('Energy Market Model - Cost Function Convergence', fontsize=14)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Cost (MSE)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Plot the fitted line using original scale\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(demand, price, alpha=0.7, label='Actual Prices')\n",
        "plt.plot(demand, our_predicted_price, color='red', linewidth=2, label='Fitted Model')\n",
        "\n",
        "# Display the equation and parameters\n",
        "equation = f'Price = {intercept:.2f} + {slope:.6f} × Demand'\n",
        "plt.annotate(equation, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
        "\n",
        "# Plot formatting\n",
        "plt.title('Day-Ahead Price vs. Demand with Linear Regression', fontsize=14)\n",
        "plt.xlabel('Demand (MW)', fontsize=12)\n",
        "plt.ylabel('Price (€/MWh)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the results\n",
        "print(\"=== Final Model Results After Normalization ===\")\n",
        "print(f\"Linear regression parameters:\")\n",
        "print(f\"Intercept (β₀): {intercept:.2f}\")\n",
        "print(f\"Slope (β₁): {slope:.6f}\")\n",
        "print(f\"Final MSE: {history['cost'][-1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76a45c41",
      "metadata": {
        "id": "76a45c41"
      },
      "source": [
        "\n",
        "> **Questions for Exercise 3:**  \n",
        "> 1. How did normalization affect convergence?  \n",
        "> 2. In your own words, **why** is z-score normalization so helpful for gradient descent?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0cc2272",
      "metadata": {
        "id": "e0cc2272"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Using scikit-learn for Linear Regression\n",
        "\n",
        "### 6.1 Introduction to scikit-learn (sklearn)\n",
        "\n",
        "[scikit-learn](https://scikit-learn.org/) is one of the most popular machine learning libraries in Python. It provides simple and efficient tools for data analysis and modeling, including:\n",
        "\n",
        "- **Classification**: SVM, nearest neighbors, random forest, etc.\n",
        "- **Regression**: Linear regression, SVR, decision trees, etc.\n",
        "- **Clustering**: K-means, DBSCAN, etc.\n",
        "- **Dimensionality reduction**: PCA, feature selection, etc.\n",
        "- **Model selection**: Grid search, cross-validation, metrics\n",
        "- **Preprocessing**: Feature extraction, normalization, encoding\n",
        "\n",
        "### 6.2 Installing scikit-learn\n",
        "\n",
        "You can install scikit-learn using either pip or conda:\n",
        "\n",
        "```bash\n",
        "# Using pip\n",
        "pip install scikit-learn\n",
        "\n",
        "# Using conda\n",
        "conda -c conda-forge scikit-learn\n",
        "```\n",
        "\n",
        "### 6.3 Linear Regression in scikit-learn\n",
        "\n",
        "The `LinearRegression` class from scikit-learn:\n",
        "- Implements ordinary least squares (OLS) linear regression\n",
        "- Has a simple API: `fit()` and `predict()`\n",
        "- Automatically calculates the optimal parameters\n",
        "- Key parameters:\n",
        "  - `fit_intercept`: Whether to calculate the intercept (default: True)\n",
        "  - `normalize`: Whether to normalize features (deprecated in newer versions)\n",
        "  - `copy_X`: Whether to copy or overwrite the X array (default: True)\n",
        "  \n",
        "Let's compare our gradient descent implementation with scikit-learn's implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c7933f",
      "metadata": {
        "id": "51c7933f"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create a LinearRegression model instance\n",
        "sklearn_model = LinearRegression()\n",
        "\n",
        "# Fit the model to our data\n",
        "# Note: sklearn expects X to be 2D array (reshape adds the second dimension)\n",
        "sklearn_model.fit(demand.reshape(-1, 1), price)\n",
        "\n",
        "# Generate predictions using sklearn model\n",
        "sklearn_predicted_price = sklearn_model.predict(demand.reshape(-1, 1))\n",
        "\n",
        "# Extract the learned parameters\n",
        "sklearn_intercept = sklearn_model.intercept_\n",
        "sklearn_slope = sklearn_model.coef_[0]\n",
        "\n",
        "# Print the comparison between our GD implementation and sklearn\n",
        "print(\"Parameter comparison:\")\n",
        "print(f\"{'Parameter':<10} {'Our GD Model':<20} {'sklearn Model':<20} {'Difference':<10}\")\n",
        "print(f\"{'-'*10:<10} {'-'*20:<20} {'-'*20:<20} {'-'*10:<10}\")\n",
        "print(f\"{'Intercept':<10} {intercept:20.5f} {sklearn_intercept:20.5f} {abs(intercept-sklearn_intercept):10.5f}\")\n",
        "print(f\"{'Slope':<10} {slope:20.5f} {sklearn_slope:20.5f} {abs(slope-sklearn_slope):10.5f}\")\n",
        "\n",
        "# Plot both models for visual comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(demand, price, alpha=0.5, label='Data Points')\n",
        "\n",
        "# Plot our GD model prediction\n",
        "plt.plot(demand, our_predicted_price, color='red', linewidth=2, label='Our Fitted Model')\n",
        "\n",
        "# Plot sklearn model prediction\n",
        "plt.plot(demand, sklearn_predicted_price,\n",
        "         color='green', linewidth=2, linestyle='--', label='sklearn LinearRegression')\n",
        "\n",
        "plt.title('Comparison: Our Gradient Descent vs. sklearn LinearRegression', fontsize=14)\n",
        "plt.xlabel('Demand (MW)', fontsize=12)\n",
        "plt.ylabel('Price (€/MWh)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04482679",
      "metadata": {
        "id": "04482679"
      },
      "source": [
        "## 7. Performance Metrics: MAE & RMSE\n",
        "\n",
        "When evaluating regression models, we need appropriate metrics. Let's explore two common metrics:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f711740",
      "metadata": {},
      "source": [
        "### 7.1 Mean Absolute Error (MAE)\n",
        "\n",
        "MAE measures the average magnitude of errors without considering their direction.\n",
        "\n",
        "**Mathematical definition**:\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y^{(i)} - \\hat{y}^{(i)}|\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $y^{(i)}$ is the actual value\n",
        "- $\\hat{y}^{(i)}$ is the predicted value\n",
        "- $n$ is the number of observations\n",
        "\n",
        "**Interpretation**:\n",
        "- MAE represents the average absolute deviation between predicted and actual values\n",
        "- It's in the same units as the target variable (e.g., €/MWh for price)\n",
        "- MAE treats all errors with equal weight (linear penalty)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a652869e",
      "metadata": {},
      "source": [
        "### 7.2 Root Mean Squared Error (RMSE)\n",
        "\n",
        "RMSE is the square root of the average of squared differences between predicted and actual values.\n",
        "\n",
        "**Mathematical definition**:\n",
        "$$\n",
        "\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y^{(i)} - \\hat{y}^{(i)})^2}\n",
        "$$\n",
        "\n",
        "**Interpretation**:\n",
        "- Like MAE, RMSE is in the same units as the target variable\n",
        "- RMSE penalizes large errors more heavily (quadratic penalty)\n",
        "- RMSE is always greater than or equal to MAE\n",
        "- A larger gap between RMSE and MAE indicates greater variance in the errors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8138d6e",
      "metadata": {},
      "source": [
        "### 7.3 Key Differences\n",
        "\n",
        "- **Sensitivity to outliers**: RMSE is more sensitive to outliers due to the squared term\n",
        "- **Interpretability**: MAE is often easier to interpret directly\n",
        "- **Mathematical properties**: RMSE has better mathematical properties for optimization\n",
        "\n",
        "Let's calculate and compare these metrics for our models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217d52a8",
      "metadata": {
        "id": "217d52a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Calculate MAE for both models\n",
        "mae_gd = mean_absolute_error(price, our_predicted_price)\n",
        "mae_sklearn = mean_absolute_error(price, sklearn_predicted_price)\n",
        "\n",
        "# Calculate RMSE for both models\n",
        "rmse_gd = np.sqrt(mean_squared_error(price, our_predicted_price))\n",
        "rmse_sklearn = np.sqrt(mean_squared_error(price, sklearn_predicted_price))\n",
        "\n",
        "# Print results in a formatted table\n",
        "print(\"Performance Metrics Comparison:\")\n",
        "print(f\"{'Model':<20} {'MAE':<10} {'RMSE':<10}\")\n",
        "print(f\"{'-'*20:<20} {'-'*10:<10} {'-'*10:<10}\")\n",
        "print(f\"{'Our Gradient Descent':<20} {mae_gd:10.2f} {rmse_gd:10.2f}\")\n",
        "print(f\"{'sklearn Linear Reg.':<20} {mae_sklearn:10.2f} {rmse_sklearn:10.2f}\")\n",
        "\n",
        "# Plot the error distributions\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Setup a 1x2 subplot grid\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(price - our_predicted_price, bins=20, alpha=0.7, color='blue')\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Error Distribution: Our GD Model', fontsize=12)\n",
        "plt.xlabel('Error (Actual - Predicted)', fontsize=10)\n",
        "plt.ylabel('Frequency', fontsize=10)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(price - sklearn_predicted_price, bins=20, alpha=0.7, color='green')\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Error Distribution: sklearn Model', fontsize=12)\n",
        "plt.xlabel('Error (Actual - Predicted)', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9666ee6b",
      "metadata": {
        "id": "9666ee6b"
      },
      "source": [
        "## 8. Multiple Linear Regression\n",
        "\n",
        "So far, we've worked with simple linear regression using a single feature. In practice, we often have multiple features that can help predict our target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p_0rXGwOOzUw",
      "metadata": {
        "id": "p_0rXGwOOzUw"
      },
      "source": [
        "### 8.1 Multiple Linear Regression Theory\n",
        "\n",
        "**Model Equation**:\n",
        "$$\n",
        "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\hat{y} $ is the predicted value\n",
        "- $\\beta_0$ is the intercept\n",
        "- $\\beta_1, \\beta_2, ..., \\beta_p$ are coefficients for each feature\n",
        "- $x_1, x_2, ..., x_p$ are the feature values\n",
        "\n",
        "In matrix notation, this becomes:\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\hat{\\mathbf{y}}$ is the vector of predictions\n",
        "- $\\mathbf{X}$ is the feature matrix (including a column of 1s for the intercept)\n",
        "- $\\boldsymbol{\\beta}$ is the vector of coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "khsn51w4O1cz",
      "metadata": {
        "id": "khsn51w4O1cz"
      },
      "source": [
        "### 8.2 Using scikit-learn for Multiple Linear Regression\n",
        "\n",
        "The `LinearRegression` class in scikit-learn handles multiple features automatically. We just need to provide a feature matrix X where each column represents a different feature.\n",
        "\n",
        "**Key Points**:\n",
        "- Feature matrix X should have shape (n_samples, n_features)\n",
        "- Target vector y should have shape (n_samples,)\n",
        "- After fitting, the coefficients are available in the `coef_` attribute as an array\n",
        "- The intercept is available in the `intercept_` attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1afc2714",
      "metadata": {
        "id": "1afc2714"
      },
      "source": [
        "### Exercise 4: Multiple Linear Regression for Energy Price Prediction\n",
        "\n",
        "**Task**: Create a multiple linear regression model to predict day-ahead energy prices using additional features from the dataset.\n",
        "\n",
        "**Steps**:\n",
        "1. Identify at least 2 additional features (besides demand) that might influence energy prices\n",
        "2. Prepare a feature matrix X with these features\n",
        "3. Train a multiple linear regression model using scikit-learn\n",
        "4. Calculate and report the MAE and RMSE\n",
        "5. Compare the performance with the single-feature model\n",
        "6. Interpret the coefficients of your model\n",
        "\n",
        "**Note**: Features you might consider include time-based features (hour of day, day of week), weather data, or other columns in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671cc913",
      "metadata": {
        "id": "671cc913"
      },
      "source": [
        "> **Note**: The following steps are provided as suggestions for the flow of the exercise. You can also implement the steps in this notebook. However, it is highly recommended to do the actual implementation in a Python file on your local machine for better practice and debugging experience.\n",
        "\n",
        "> **Reminder**: To install `scikit-learn` using `conda`, you can use the following command:\n",
        "> ```bash\n",
        "> conda -c conda-forge scikit-learn\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b11808",
      "metadata": {
        "id": "98b11808"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset and identify features you want to use\n",
        "# Explore the available columns in the dataset\n",
        "\n",
        "print(\"Available columns in the dataset:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# TODO: Select your features and create a feature matrix\n",
        "# Your code here\n",
        "# X_multi = df[['demand', 'feature2', 'feature3']].values\n",
        "\n",
        "# Target variable remains the same\n",
        "price = df['DA Price'].values\n",
        "\n",
        "# Display the first few rows of your selected features\n",
        "print(\"Preview of selected features:\")\n",
        "# print(X_multi[:5, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b59588",
      "metadata": {
        "id": "60b59588"
      },
      "outputs": [],
      "source": [
        "# Step 2: Train a multiple linear regression model\n",
        "\n",
        "# TODO: Create and fit a linear regression model using multiple features\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# TODO: Extract and print the coefficients and intercept\n",
        "# Your code here\n",
        "# print(f\"Intercept: {multi_model.intercept_:.4f}\")\n",
        "# print(\"Coefficients:\")\n",
        "# for feature, coef in zip(['demand', 'feature2', 'feature3'], multi_model.coef_):\n",
        "#     print(f\"{feature}: {coef:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d594980c",
      "metadata": {
        "id": "d594980c"
      },
      "outputs": [],
      "source": [
        "# Step 3: Evaluate the multiple regression model\n",
        "\n",
        "# TODO: Generate predictions using your multiple regression model\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# TODO: Calculate MAE and RMSE\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# TODO: Compare with the single-feature model\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea67f23",
      "metadata": {
        "id": "5ea67f23"
      },
      "outputs": [],
      "source": [
        "# Step 4: Visualize the results\n",
        "\n",
        "# TODO: Create visualizations to compare your models\n",
        "# Your code here\n",
        "\n",
        "# Example visualization: Actual vs Predicted plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6807e80d",
      "metadata": {
        "id": "6807e80d"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations on completing this notebook!\n",
        "\n",
        "You’ve now walked through—and hands-on—every step of building, debugging, and evaluating a simple linear regression model:\n",
        "\n",
        "* **Why and how gradient descent works**: you derived the update rules and saw how the choice of learning rate shapes convergence.\n",
        "* **The importance of feature scaling**: by first running on raw data, you observed divergence; then, after implementing z-score normalization yourself, you achieved smooth, reliable convergence.\n",
        "* **Real-world application**: you applied your code to day-ahead electricity market data, learned how to denormalize parameters, and visualized the fitted model against actual prices.\n",
        "* **Comparisons and metrics**: you contrasted your implementation with scikit-learn’s OLS solution and used MAE vs. RMSE to understand different error perspectives.\n",
        "* **Growth to multiple regression**: you’re now set up to include extra features to capture more complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### Reflect Before You Leave\n",
        "\n",
        "- What did you learn about the role of feature scaling in ML?\n",
        "- What were the strengths and weaknesses of your gradient descent implementation?\n",
        "- Could this model be improved with non-linear features or domain-specific constraints?\n",
        "\n",
        "Write 3–5 bullet points summarizing your insights."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "assume-framework",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
