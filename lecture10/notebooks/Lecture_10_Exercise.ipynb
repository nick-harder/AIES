{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0e33",
   "metadata": {},
   "source": [
    "# Lecture 10 – Clustering Exercises\n",
    "\n",
    "**Welcome to your hands-on notebook for Lecture 10!**\n",
    "\n",
    "**Motivation:** Clustering load profiles reveals groups of similar energy usage patterns, enabling applications like demand response aggregation and DER detection in energy systems.\n",
    "\n",
    "In this exercise notebook you will:\n",
    "1. **Load and explore** 15-minute smart-meter data\n",
    "2. **Engineer features** (domain-agnostic & domain-informed) and **Standardize** the feature matrix  \n",
    "3. **Visualize** the feature space with t-SNE and PCA\n",
    "4. **Cluster** load profiles using K-Means and Agglomerative (Ward)  \n",
    "5. **Evaluate** clusters via silhouette scores and inertia  \n",
    "6. **Interpret** cluster characteristics in terms of DER presence  \n",
    "7. **Reflect** on your results and plan next steps  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ea0d1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We begin by importing all Python packages needed throughout this notebook:\n",
    "\n",
    "- **pandas**, **numpy** for data handling  \n",
    "- **matplotlib** for plotting  \n",
    "- **sklearn** modules for preprocessing, dimensionality reduction, clustering, and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# packages for files handling and downloading\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# packages for data processing and machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, classification_report\n",
    "from scipy.stats import kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91face2",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "Load the CSV of 15-minute consumption/injection data, inspect its shape, and plot two sample meters to get a feel for raw load profiles.\n",
    "\n",
    "This dataset is part of the corpus of quarter hourly data from 1,300 residential buildings in Flanders (Belgium) made openly available for the year 2022 by Fluvius, the Flemish DSO. We will be using parts of this dataset during our lecture 10 through 12. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14479e",
   "metadata": {},
   "source": [
    "**This cell will create an `inputs/` folder (if it doesn't already exist) and then download the `sample_smart_meter_data.parquet` file from the GitHub repository into that folder.**\n",
    "\n",
    "> Note: Eventhough this is only a small part of the full dataset, it is still quite large and storing it in a CSV format would be inefficient. Therefore, we use the Parquet format, which is more efficient for large datasets. To read more about the Parquet format and how to load and save it, you can refer to Pandas documentation on [Parquet](https://pandas.pydata.org/docs/user_guide/io.html#parquet). But in a nutshell, Parquet is a columnar storage file format that is optimized for use with big data processing frameworks like Apache Spark and Hadoop. It is designed to be efficient for both reading and writing large datasets, and it supports advanced features like compression and schema evolution. And once it is loaded, you can work with it just like a regular Pandas DataFrame.\n",
    "\n",
    "If you are running this notebook in Google Colab, Pyarrow is already installed, so you can directly read the Parquet file. If you are running this notebook locally, you may need to install the `pyarrow` package to read Parquet files. You can do this by running `pip install pyarrow` in your terminal or command prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create the 'inputs' folder if it doesn't exist\n",
    "os.makedirs('inputs', exist_ok=True)\n",
    "\n",
    "# 2) Define the URL to the raw CSV on GitHub and the local path\n",
    "url = 'https://raw.githubusercontent.com/nick-harder/AIES/main/lecture10/data/sample_smart_meter_data.parquet'\n",
    "local_path = 'inputs/sample_smart_meter_data.parquet'\n",
    "\n",
    "# 3) Download the file only if it's not already present\n",
    "if not os.path.exists(local_path):\n",
    "    print(f\"Downloading data from {url} ...\")\n",
    "    urllib.request.urlretrieve(url, local_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Load\n",
    "df = pd.read_parquet(local_path).reset_index()\n",
    "# parse timestamps\n",
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "\n",
    "# 1.2 Overview\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# 1.3 Plot two sample meters\n",
    "sample_ids = df[\"Meter_ID\"].unique()[:2]\n",
    "plt.figure(figsize=(12,4))\n",
    "for meter in sample_ids:\n",
    "    series = df[df[\"Meter_ID\"]==meter].set_index(\"Timestamp\")[\"Consumption_kWh\"]\n",
    "    # select only April 2023 data\n",
    "    series = series[series.index.month == 4]\n",
    "    plt.plot(series.index, series.values, label=f\"Meter {meter}\")\n",
    "plt.legend()\n",
    "plt.title(\"Raw Load Profiles for Two Sample Meters\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Consumption (kWh)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3519a6",
   "metadata": {},
   "source": [
    "We also fix the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75820e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Domain-Agnostic Feature Extraction\n",
    "\n",
    "We compute basic statistical features over each meter’s full time series and link them to physical insights:\n",
    "\n",
    "- **Mean**: average consumption (baseline usage level)  \n",
    "- **Standard deviation**: variability (load volatility)  \n",
    "- **Skewness**: asymmetry (peak vs. off-peak behavior)  \n",
    "- **Kurtosis**: peakedness (extreme events)  \n",
    "- **Autocorrelation** at lag 1 and lag 24: temporal persistence (daily repeatability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean(x):\n",
    "    return x.mean()\n",
    "\n",
    "def compute_std(x):\n",
    "    return x.std()\n",
    "\n",
    "def compute_skewness(x):\n",
    "    return x.skew()\n",
    "\n",
    "def impute_missing(x):\n",
    "    return x.ffill().bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceaf7ea",
   "metadata": {},
   "source": [
    "### 3.1 Exercise 1: Implement Kurtosis and Autocorrelation\n",
    "Implement the kurtosis and autocorrelation features in the `extract_features` function. Use `scipy.stats.kurtosis` for kurtosis and `pandas.Series.autocorr` for autocorrelation. Ensure you compute autocorrelation for both lag 1 and lag 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4422c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kurtosis(x):\n",
    "    # TODO: implement kurtosis (e.g. x.kurtosis())\n",
    "    raise NotImplementedError(\"Compute kurtosis here\")\n",
    "\n",
    "def compute_autocorr(x, lag):\n",
    "    # TODO: implement autocorrelation (e.g. x.autocorr(lag=lag))\n",
    "    raise NotImplementedError(\"Compute autocorrelation here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec661c01",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Domain-Informed Feature Extraction\n",
    "\n",
    "These metrics capture patterns linked to DER presence (PV panels, EV charging):\n",
    "\n",
    "- **Midday Dip**: drop around 12–14 h (PV generation reduces net consumption)  \n",
    "- **Evening Ramp**: increase 18–21 h vs. 15–18 h (EV charging starts)  \n",
    "- **Night/Day Ratio**: avg. load 00–06 h ÷ 06–18 h (overnight appliances)  \n",
    "- **Weekend Load Factor**: avg. weekend ÷ weekday (occupancy patterns)  \n",
    "- **Peak-to-Average Ratio**: peak demand ÷ mean (device spikes)  \n",
    "- **Longest Period Above Mean**: continuous high-load streaks (e.g. heating)  \n",
    "- **Longest Period of Successive Increase**: sustained rises (e.g. EV charging session)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33334781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_midday_dip(x):\n",
    "    \"\"\"\n",
    "    Measures how much the consumption drops during midday hours (12:00–14:00)\n",
    "    relative to the overall average.\n",
    "\n",
    "    A pronounced dip could suggest solar PV generation reducing net load.\n",
    "    \"\"\"\n",
    "    midday = x.between_time(\"12:00\", \"14:00\").mean()\n",
    "    return midday - x.mean()\n",
    "\n",
    "\n",
    "def compute_evening_ramp(x):\n",
    "    \"\"\"\n",
    "    Measures the increase in consumption from afternoon (15:00–18:00)\n",
    "    to evening hours (18:00–21:00).\n",
    "\n",
    "    A steep evening ramp may indicate the start of EV charging or cooking activity.\n",
    "    \"\"\"\n",
    "    eve = x.between_time(\"18:00\", \"21:00\").mean()\n",
    "    aft = x.between_time(\"15:00\", \"18:00\").mean()\n",
    "    return eve - aft\n",
    "\n",
    "\n",
    "def compute_night_day_ratio(x):\n",
    "    \"\"\"\n",
    "    Computes the ratio of average night-time load (00:00–06:00)\n",
    "    to average day-time load (06:00–18:00).\n",
    "\n",
    "    This feature helps detect households with high overnight usage (e.g., EV charging).\n",
    "    \"\"\"\n",
    "    night = x.between_time(\"00:00\", \"06:00\").mean()\n",
    "    day = x.between_time(\"06:00\", \"18:00\").mean()\n",
    "    return night / day\n",
    "\n",
    "\n",
    "def compute_weekend_load_factor(x):\n",
    "    \"\"\"\n",
    "    Calculates the ratio of weekend average load to weekday average load.\n",
    "\n",
    "    Useful to detect occupancy patterns—e.g., higher weekend usage could indicate\n",
    "    a home-based lifestyle or weekend charging.\n",
    "    \"\"\"\n",
    "    wknd = x[x.index.dayofweek >= 5].mean()  # Saturday=5, Sunday=6\n",
    "    wkday = x[x.index.dayofweek < 5].mean()  # Monday–Friday\n",
    "    return wknd / wkday\n",
    "\n",
    "\n",
    "def compute_peak_to_avg(x):\n",
    "    \"\"\"\n",
    "    Ratio of peak load to average load across the entire time series.\n",
    "\n",
    "    High values indicate \"spikiness\"—typically associated with event-driven loads like EV charging.\n",
    "    \"\"\"\n",
    "    return x.max() / x.mean()\n",
    "\n",
    "\n",
    "def compute_longest_above_mean(x):\n",
    "    \"\"\"\n",
    "    Computes the longest consecutive time period during which consumption remains\n",
    "    above the mean level.\n",
    "\n",
    "    Long high-usage periods may indicate appliances with sustained load, such as HVAC or EVs.\n",
    "    \"\"\"\n",
    "    threshold = x.mean()\n",
    "    mask = x > threshold\n",
    "\n",
    "    # If no values exceed the mean, return 0\n",
    "    if not mask.any():\n",
    "        return 0\n",
    "\n",
    "    # Identify contiguous runs above the threshold\n",
    "    runs = (mask != mask.shift()).cumsum()\n",
    "    lengths = mask.groupby(runs).sum()\n",
    "    return int(lengths.max())\n",
    "\n",
    "\n",
    "def compute_longest_increase_streak(x):\n",
    "    \"\"\"\n",
    "    Finds the longest sequence of strictly increasing consumption values\n",
    "    in the time series.\n",
    "\n",
    "    Can indicate gradual ramp-up behavior, such as slow EV charging or heat pump cycles.\n",
    "    \"\"\"\n",
    "    delta = x.diff()\n",
    "    mask = delta > 0\n",
    "\n",
    "    if not mask.any():\n",
    "        return 0\n",
    "\n",
    "    runs = (mask != mask.shift()).cumsum()\n",
    "    lengths = mask.groupby(runs).sum()\n",
    "    return int(lengths.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a38fc5",
   "metadata": {},
   "source": [
    "## 5. Prepare Features\n",
    "\n",
    "### 5.1 Build Feature Matrix\n",
    "\n",
    "Apply both sets of functions to each meter to assemble a feature DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29131849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactored: modular feature extraction\n",
    "def extract_features(df, meter_id):\n",
    "    series = df[df[\"Meter_ID\"]==meter_id].set_index(\"Timestamp\")[\"Consumption_kWh\"]\n",
    "    series = impute_missing(series)\n",
    "    return {\n",
    "        \"Meter_ID\": meter_id,\n",
    "        \"mean\": compute_mean(series),\n",
    "        \"std\": compute_std(series),\n",
    "        \"skew\": compute_skewness(series),\n",
    "        \"kurtosis\": compute_kurtosis(series),\n",
    "        \"autocorr_lag1\": compute_autocorr(series,1),\n",
    "        \"autocorr_lag24\": compute_autocorr(series,24),\n",
    "        \"midday_dip\": compute_midday_dip(series),\n",
    "        \"evening_ramp\": compute_evening_ramp(series),\n",
    "        \"night_day_ratio\": compute_night_day_ratio(series),\n",
    "        \"weekend_load_factor\": compute_weekend_load_factor(series),\n",
    "        \"peak_to_avg\": compute_peak_to_avg(series),\n",
    "        \"longest_above_mean\": compute_longest_above_mean(series),\n",
    "        \"longest_increase_streak\": compute_longest_increase_streak(series),\n",
    "    }\n",
    "\n",
    "meters = df[\"Meter_ID\"].unique() # Get unique meter IDs\n",
    "feature_list = [extract_features(df, m) for m in meters] # Extract features for each meter\n",
    "\n",
    "feature_df = pd.DataFrame(feature_list).set_index(\"Meter_ID\") # Convert to DataFrame\n",
    "\n",
    "print(\"Feature DataFrame shape:\", feature_df.shape) # Display shape of feature DataFrame\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7497a",
   "metadata": {},
   "source": [
    "### 5.2 Feature Standardization\n",
    "\n",
    "Normalize each feature to zero mean and unit variance for fair clustering. This ensures all features contribute equally to distance calculations. This is important for clustering algorithms like K-Means that are sensitive to feature scales.\n",
    "\n",
    "> Info: If the data is not standardized, features with larger ranges can disproportionately influence the clustering results, leading to biased clusters. Imagine if one feature ranged from 0 to 1000 while another ranged from 0 to 1; the first feature would dominate the distance calculations, skewing the clusters. Below you can see an example plot of such two features before and after standardization and the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff42b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Demonstration of Feature Scaling on KMeans Clustering\n",
    "\n",
    "# --- Generate Synthetic Data with Distinct Clouds ---\n",
    "np.random.seed(random_seed)\n",
    "n_points = 30\n",
    "centers = [(80, 0.3), (90, 0.7), (100, 0.5)]\n",
    "clusters = []\n",
    "for cx, cy in centers:\n",
    "    cluster = np.random.normal(loc=(cx, cy), scale=(5, 0.05), size=(n_points, 2))\n",
    "    clusters.append(cluster)\n",
    "features = np.vstack(clusters)\n",
    "\n",
    "# --- KMeans on Raw Data ---\n",
    "kmeans_raw = KMeans(n_clusters=3, random_state=random_seed).fit(features)\n",
    "labels_raw = kmeans_raw.labels_\n",
    "\n",
    "# --- Standardize Features ---\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# --- KMeans on Standardized Data ---\n",
    "kmeans_scaled = KMeans(n_clusters=3, random_state=random_seed).fit(features_scaled)\n",
    "labels_scaled = kmeans_scaled.labels_\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before Standardization\n",
    "axs[0].scatter(features[:, 0], features[:, 1], c=labels_raw, alpha=0.6)\n",
    "axs[0].set_title('KMeans Clusters before Standardization')\n",
    "axs[0].set_xlabel('Feature 1')\n",
    "axs[0].set_ylabel('Feature 2')\n",
    "\n",
    "# After Standardization\n",
    "axs[1].scatter(features_scaled[:, 0], features_scaled[:, 1], c=labels_scaled, alpha=0.6)\n",
    "axs[1].set_title('KMeans Clusters after Standardization')\n",
    "axs[1].set_xlabel('Standardized Feature 1')\n",
    "axs[1].set_ylabel('Standardized Feature 2')\n",
    "\n",
    "plt.suptitle('Effect of Feature Scaling on KMeans Clustering', fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c804b8",
   "metadata": {},
   "source": [
    "In the following code cell, we standardize the feature matrix using `sklearn.preprocessing.StandardScaler`. This will ensure that each feature has a mean of 0 and a standard deviation of 1, making them comparable in the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64194f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # Initialize StandardScaler\n",
    "scaled_arr = scaler.fit_transform(feature_df) # Fit and transform the feature DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_arr, index=feature_df.index, columns=feature_df.columns) # Create a new DataFrame with scaled values\n",
    "\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767daaea",
   "metadata": {},
   "source": [
    "### 5.3 t-SNE Visualization\n",
    "\n",
    "Project the high-dimensional feature space into 2D to explore natural groupings. For this, we use t-SNE with a perplexity of 30 and 1000 iterations. This will help us visualize the structure of the data and identify potential clusters. To read more about t-SNE, you can refer to the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) or here in a [medium article](https://medium.com/data-science/t-sne-clearly-explained-d84c537f53a).\n",
    "\n",
    "This doesn't give us clusters directly, but it helps us see how the data points are distributed in a lower-dimensional space. And maybe you can already spot some clusters visually?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951adbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=random_seed)\n",
    "coords = tsne.fit_transform(scaled_df)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(coords[:,0], coords[:,1], alpha=0.7)\n",
    "plt.title(\"t-SNE Projection of Building Features\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30df9e",
   "metadata": {},
   "source": [
    "### 5.4 Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a **linear** dimensionality‐reduction technique that finds a new orthogonal basis maximizing the variance in the data. In practice:\n",
    "\n",
    "1. **Purpose**  \n",
    "   - Reduce high-dimensional data to 2D for visualization.  \n",
    "   - Quantify how much variance each principal component (PC) captures.\n",
    "\n",
    "2. **Why It Matters**  \n",
    "   - Unlike t-SNE (which excels at revealing local clusters), PCA gives a **global** picture of variance structure.  \n",
    "   - Pre-processing with PCA can denoise data and speed up downstream methods (e.g. t-SNE, clustering).\n",
    "\n",
    "> **Tip:** Always inspect the `explained_variance_ratio_` to see how much “signal” you’re retaining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the scaled data\n",
    "pca = PCA(n_components=2, random_state=random_seed)\n",
    "pca_coords = pca.fit_transform(scaled_df)\n",
    "\n",
    "# Print explained variance\n",
    "explained = pca.explained_variance_ratio_\n",
    "print(f\"PC1 explains {explained[0]:.1%} variance; PC2 explains {explained[1]:.1%}\")\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(pca_coords[:,0], pca_coords[:,1], alpha=0.7)\n",
    "plt.title(\"PCA Projection of Building Features\")\n",
    "plt.xlabel(f\"PC1 ({explained[0]:.1%})\")\n",
    "plt.ylabel(f\"PC2 ({explained[1]:.1%})\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0749b6a",
   "metadata": {},
   "source": [
    "## 6. Clustering with K-Means\n",
    "\n",
    "Clustering using K-Means is a powerful way to group similar load profiles. We will explore how many clusters to use by evaluating silhouette scores across different values of `k`. We will also use the inertia to help us determine the optimal number of clusters. The difference between inertia and silhouette score is that inertia measures how tightly the clusters are packed, while silhouette score measures how well-separated the clusters are.\n",
    "\n",
    "For clustering, we will use the `KMeans` algorithm from `sklearn.cluster`. We will loop over a range of `k` values (2 to 8) and compute the silhouette score for each. The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters.\n",
    "\n",
    "After the clustering, we will visualize the clusters using t-SNE to see how well the K-Means algorithm has grouped the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fbd54",
   "metadata": {},
   "source": [
    "### 6.1 Exercise 2: K-Means Clustering and Scoring\n",
    "In this exercise, you will implement K-Means clustering and evaluate the results using silhouette scores:\n",
    "\n",
    "1. Loop over `k = 2…8`.  \n",
    "2. For each `k`, fit a `KMeans(n_clusters=k, random_state=random_seed)` model on your **scaled** feature matrix.  \n",
    "3. Compute the silhouette score with `silhouette_score(X, labels)`.  \n",
    "4. Compute the inertia with `kmeans.inertia_`.\n",
    "5. Collect all scores in a list and print them.\n",
    "6. Plot the silhouette scores and inertia against `k` to visualize the “elbow” and identify the optimal number of clusters.\n",
    "\n",
    "Hints:  \n",
    "- Use `sklearn.cluster.KMeans` to get `labels = kmeans.fit_predict(...)`.  \n",
    "- Use `sklearn.metrics.silhouette_score` to evaluate.  \n",
    "- You can visualize the “elbow” by plotting `ks` on the x-axis and your score list on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "inertia_scores = []\n",
    "ks = range(2, 9)\n",
    "\n",
    "for k in ks:\n",
    "    # TODO: Create a KMeans object with k clusters and a fixed random_state\n",
    "    kmeans = ...\n",
    "\n",
    "    # TODO: Fit the model and predict cluster labels\n",
    "    labels = ...\n",
    "\n",
    "    # TODO: Compute silhouette score and append to the list\n",
    "    score = ...\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "    # TODO: Append the model's inertia to the list\n",
    "    inertia_scores.append(...)\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"k = {k}, silhouette score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a617579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot silhouette vs. k\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(list(ks), silhouette_scores, marker='o', linestyle='-')\n",
    "plt.xticks(list(ks))\n",
    "plt.xlabel('Number of clusters k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette Analysis for K-Means')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot inertia vs. k\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(list(ks), inertia_scores, marker='o', linestyle='-')\n",
    "plt.xticks(list(ks))\n",
    "plt.xlabel('Number of clusters k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Inertia for K-Means Clustering')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61712c11",
   "metadata": {},
   "source": [
    "Now let's visualize the clusters using t-SNE. This will help us see how well the K-Means algorithm has grouped the data points. We will use two clusters for simplicity, but you can experiment with more clusters if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means with k=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=random_seed)\n",
    "km_labels = kmeans.fit_predict(scaled_df)\n",
    "\n",
    "# silhouette score\n",
    "score = silhouette_score(scaled_df, km_labels)\n",
    "print(f\"KMeans (k=2) silhouette: {score:.3f}\")\n",
    "\n",
    "# t-SNE scatter colored by KMeans labels\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(coords[:,0], coords[:,1], c=km_labels, cmap='tab10', alpha=0.7)\n",
    "plt.title(\"t-SNE Colored by KMeans k=2\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049288a",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Agglomerative Clustering (Ward)\n",
    "\n",
    "We apply hierarchical clustering with Ward linkage for $k=2$, compare silhouette and visualize on t-SNE.\n",
    "\n",
    "To do this, we will use the `AgglomerativeClustering` class from `sklearn.cluster`. This class allows us to perform hierarchical clustering using different linkage criteria. We will use the Ward method, which minimizes the variance of clusters being merged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2855f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering with Ward linkage with k=2\n",
    "ward = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
    "agg_labels = ward.fit_predict(scaled_df)\n",
    "score = silhouette_score(scaled_df, agg_labels)\n",
    "print(f\"Ward (k=2) silhouette: {score:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(coords[:,0], coords[:,1], c=agg_labels, cmap='tab10', alpha=0.7)\n",
    "plt.title(f\"t-SNE Colored by Ward k=2\")\n",
    "plt.xlabel(\"TSNE 1\")\n",
    "plt.ylabel(\"TSNE 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212fd89",
   "metadata": {},
   "source": [
    "## 8. Cluster Interpretation\n",
    "\n",
    "Examine average feature values per cluster to infer DER presence. Do this for both the K-Means and Agglomerative clusters. This will help us understand the characteristics of each cluster and how they relate to DER presence.\n",
    "\n",
    ">Note: Pay attention to the same values as we have identified in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the original feature DataFrame and add cluster labels\n",
    "clustered = scaled_df.copy()\n",
    "\n",
    "# KMeans with k=2\n",
    "clustered['km_cluster'] = km_labels\n",
    "cluster_means = clustered.groupby('km_cluster').mean()\n",
    "\n",
    "# plot KMeans cluster means\n",
    "cluster_means.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"KMeans Cluster Means\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Mean Feature Value\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Agglomerative with k=2\n",
    "clustered['agg_cluster'] = agg_labels\n",
    "agg_cluster_means = clustered.groupby('agg_cluster').mean()\n",
    "\n",
    "# plot Agglomerative cluster means\n",
    "agg_cluster_means.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Agglomerative Cluster Means\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Mean Feature Value\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905797a",
   "metadata": {},
   "source": [
    "## 9. Assign and Evaluate Your Clusters\n",
    "\n",
    "Now that you’ve explored and clustered your buildings, your final task is to **decide which cluster** corresponds to pure Baseline consumers and which one to EV-Only households. Write down your cluster assumptions and run the evaluation code below to see how well your clustering matches the true categories.\n",
    "\n",
    "- **Step 1:** Inspect the t-SNE and feature summaries to decide which cluster corresponds to Baseline and which to EV-Only.\n",
    "- **Step 2:** Plug your mapping into the code and run to compute overall accuracy and view the confusion matrix.  \n",
    "- **Step 3:** Reflect on any mis-classified meters—what drove the errors, and how might you improve your features or clustering?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb320be8",
   "metadata": {},
   "source": [
    "### 9.1 Evaluation Utilities: Cluster Assignment Accuracy\n",
    "\n",
    "To avoid duplicating code, we define a reusable function that evaluates your clustering results against ground-truth labels. You simply pass in:\n",
    "\n",
    "- `cluster_series`: a pandas Series of predicted cluster IDs (indexed by Meter_ID)  \n",
    "- `true_labels`: the ground-truth Series of meter categories (`Baseline`, `EV_Only`)  \n",
    "- `cluster_to_label`: a dictionary mapping your cluster IDs to the actual label names\n",
    "\n",
    "The function computes:\n",
    "\n",
    "- Accuracy  \n",
    "- Confusion matrix  \n",
    "- Classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(cluster_series, true_labels, cluster_to_label, method_name=\"Clustering\"):\n",
    "    \"\"\"\n",
    "    Evaluate a clustering against true labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_series: pd.Series of cluster IDs (indexed by Meter_ID)\n",
    "    - true_labels: pd.Series of true labels (indexed by Meter_ID)\n",
    "    - cluster_to_label: dict mapping {cluster_id: \"Category_Label\"}\n",
    "    - method_name: str for display title\n",
    "    \"\"\"\n",
    "    # Map cluster IDs to predicted category labels\n",
    "    predicted_labels = cluster_series.map(cluster_to_label)\n",
    "    \n",
    "    # Align and mask\n",
    "    y_true = true_labels.reindex(predicted_labels.index)\n",
    "    y_pred = predicted_labels\n",
    "    mask = y_true.notna()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = (y_pred[mask] == y_true[mask]).mean()\n",
    "    print(f\"\\n{method_name} Accuracy (on {mask.sum()} labeled meters): {accuracy*100:.2f}%\\n\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf = pd.crosstab(y_pred[mask], y_true[mask],\n",
    "                       rownames=[f\"Predicted ({method_name})\"], colnames=[\"True\"])\n",
    "    print(f\"Confusion Matrix ({method_name}):\")\n",
    "    print(conf)\n",
    "\n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report ({method_name}):\")\n",
    "    print(classification_report(y_true[mask], y_pred[mask], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040d596",
   "metadata": {},
   "source": [
    "We will also download the ground truth labels from the GitHub repository. This will allow us to evaluate our clustering results against the true categories of Baseline and EV-Only households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the URL to the raw CSV on GitHub and the local path\n",
    "url = 'https://raw.githubusercontent.com/nick-harder/AIES/main/lecture10/data/metadata_labeled.csv'\n",
    "local_path = 'inputs/metadata_labeled.csv'\n",
    "\n",
    "# 3) Download the file only if it's not already present\n",
    "if not os.path.exists(local_path):\n",
    "    print(f\"Downloading data from {url} ...\")\n",
    "    urllib.request.urlretrieve(url, local_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already exists, skipping download.\")\n",
    "\n",
    "# Load true labels if not already loaded\n",
    "true_labels = pd.read_csv(\"inputs/metadata_labeled.csv\", index_col=\"Meter_ID\")[\"Category_Label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098dde28",
   "metadata": {},
   "source": [
    "### 9.2 Exercise 3: Decide Cluster Assignments and Evaluate\n",
    "\n",
    "In this exercise, you will assign your clusters to the Baseline and EV-Only categories and evaluate the results:\n",
    "\n",
    "1. **Decide which cluster corresponds to Baseline and which to EV-Only** based on your analysis of the t-SNE plot and feature summaries.\n",
    "2. **Update Cluster Maps** to your cluster assignments in the below code cells. \n",
    "3. **Run the evaluation function** with your cluster assignments and the ground-truth labels to compute accuracy and confusion matrix.\n",
    "4. **Reflect on the results**: How well did your clustering match the true categories? What were the main sources of error? How might you improve your feature extraction or clustering approach in the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff11d0",
   "metadata": {},
   "source": [
    "#### 9.2.1 Evaluate K-Means Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your K-Means cluster mapping\n",
    "cluster_map_km = {\n",
    "    None: \"Baseline\", # TODO: define a baseline cluster\n",
    "    None: \"EV_Only\" # TODO: define an EV Only cluster\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "evaluate_clustering(clustered[\"km_cluster\"], true_labels, cluster_map_km, method_name=\"K-Means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845da24",
   "metadata": {},
   "source": [
    "#### 9.2.2 Evaluate Agglomerative Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Agglomerative cluster mapping\n",
    "cluster_map_ward = {\n",
    "    None: \"Baseline\", # TODO: define a baseline cluster\n",
    "    None: \"EV_Only\" # TODO: define an EV Only cluster\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "evaluate_clustering(clustered[\"agg_cluster\"], true_labels, cluster_map_ward, method_name=\"Agglomerative\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d446ca",
   "metadata": {},
   "source": [
    "### 9.3 Understanding the Classification Report\n",
    "\n",
    "After mapping your cluster assignments to real-world categories (e.g., `Baseline`, `EV_Only`), you evaluated the clustering performance using `sklearn.metrics.classification_report`.\n",
    "\n",
    "This report provides a detailed breakdown of how well your clustering matches the ground truth. Here's how to interpret the key metrics shown in the report:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Precision**\n",
    "- **Definition**: Of all buildings predicted to belong to a class (e.g., EV_Only), how many truly belong to that class?\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "- **High precision** means few false positives.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Recall**\n",
    "- **Definition**: Of all buildings that truly belong to a class, how many were correctly predicted?\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "- **High recall** means few false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "**3. F1-Score**\n",
    "- **Definition**: The harmonic mean of precision and recall. It balances both.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "- Useful when you want a single metric that considers both types of error.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Support**\n",
    "- **Definition**: The number of true instances for each class in the dataset.\n",
    "- This gives context to the above scores—how many examples were available for each label.\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:** Review both the confusion matrix and classification report to identify which classes are being misclassified and whether errors are systematic (e.g., always mislabeling EVs as Baseline).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbe5ed",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "This notebook guided you through a complete workflow for identifying patterns in household electricity consumption using unsupervised learning techniques. The goal was to cluster buildings based on their load profiles and uncover behavioral patterns such as the presence of electric vehicles or photovoltaic systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Concepts and Skills Covered\n",
    "\n",
    "1. Data Exploration and Preprocessing\n",
    "- You loaded high-frequency smart meter data and visualized consumption over time for individual buildings.\n",
    "- You addressed missing values and ensured time alignment for analysis.\n",
    "\n",
    "2. Feature Engineering\n",
    "- You computed both domain-agnostic features (mean, standard deviation, skewness, kurtosis, autocorrelation) and domain-informed features (midday dip, evening ramp, weekend load factor, etc.).\n",
    "- This dual approach enabled you to capture both statistical patterns and application-specific energy usage characteristics.\n",
    "\n",
    "3. Dimensionality Reduction with t-SNE\n",
    "- You used t-SNE to visualize the high-dimensional feature space in two dimensions.\n",
    "- This allowed you to qualitatively assess whether natural groupings existed in the data.\n",
    "\n",
    "4. Clustering\n",
    "- You implemented and evaluated K-Means and Agglomerative (Ward) clustering.\n",
    "- Silhouette scores and t-SNE visualizations helped assess cluster quality and separation.\n",
    "\n",
    "5. Evaluation and Interpretation\n",
    "- You compared your predicted clusters against labeled ground-truth categories.\n",
    "- By analyzing confusion matrices and classification reports, you quantified how well your unsupervised methods aligned with known DER patterns.\n",
    "- You reflected on how specific features contributed to cluster separation and what characteristics define each group.\n",
    "\n",
    "---\n",
    "\n",
    "### What You Have Learned\n",
    "\n",
    "- How to engineer effective features from raw time series data\n",
    "- The importance of both statistical and domain-specific insights when preparing data\n",
    "- How to apply and compare different clustering techniques\n",
    "- How to interpret clustering outputs in the context of real-world systems\n",
    "- How to validate unsupervised learning results using labeled metadata\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the final section, you were invited to analyze cluster behavior and assign real-world interpretations based on feature characteristics. You also evaluated your clustering performance against known labels.\n",
    "\n",
    "If you want to continue learning and improving:\n",
    "\n",
    "- Experiment with new features (e.g., Fourier coefficients, ramp rates, injection patterns)\n",
    "- Try different dimensionality reduction methods like PCA or UMAP\n",
    "- Use other clustering methods such as DBSCAN or Gaussian Mixture Models\n",
    "- Incorporate external data (e.g., weather, occupancy) into your feature set\n",
    "\n",
    "This notebook should serve as a strong foundation for applying unsupervised learning in practical energy analytics settings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
