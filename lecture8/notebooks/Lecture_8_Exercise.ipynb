{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4157dc",
   "metadata": {},
   "source": [
    "# Lecture 8: Non-linear Methods\n",
    "\n",
    "Welcome to **Lecture 8**! Building on what we learned in Lecture 7, we will now explore more advanced **non-linear regression techniques** for energy systems data. We'll continue using the same dataset, but shift our focus to powerful **tree-based models**: **Random Forests** and **Gradient Boosting (LightGBM)**.\n",
    "\n",
    "By the end of this notebook, you’ll know how to:\n",
    "\n",
    "- Analyze feature correlations to inform model input selection\n",
    "- Perform robust data splitting for training, validation, and testing\n",
    "- Train and evaluate tree-based regression models\n",
    "- Compare non-linear models with linear regression\n",
    "- Interpret feature importance and model behavior\n",
    "\n",
    "This notebook is designed for both **self-study and in-class exercises**.  \n",
    "\n",
    "**Watch for the 'Exercise' sections to apply what you’ve learned!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11a622",
   "metadata": {},
   "source": [
    "### ML Pipeline Overview\n",
    "\n",
    "In this session, we’ll follow a **simple, repeatable pipeline** for any supervised learning task:\n",
    "\n",
    "1. **Explore & Prepare Data**  \n",
    "2. **Split** into train / validation / test  \n",
    "3. **Train** one or more models  \n",
    "4. **Evaluate** on validation and test sets  \n",
    "5. **Interpret** results (feature importance, errors, plots)  \n",
    "6. **Save** trained models for future use  \n",
    "\n",
    "As we move through each exercise, you’ll see exactly where it fits in this pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b4457",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let’s start by importing the libraries we’ll use throughout this notebook:\n",
    "\n",
    "- `pandas` and `numpy`: Data manipulation and numerical operations\n",
    "- `matplotlib.pyplot` and `seaborn`: Data visualization\n",
    "- `scikit-learn`: Tools for model evaluation, splitting, metrics, and Random Forests\n",
    "- `lightgbm`: Fast and efficient gradient boosting framework\n",
    "\n",
    "If LightGBM is not installed on your system, you can install it with:\n",
    "\n",
    "```bash\n",
    "pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LightGBM if not already installed\n",
    "%pip install lightgbm\n",
    "# Install seaborn for more advanced visualization\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cbe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Handling ---\n",
    "import pandas as pd  # For loading, manipulating, and analyzing structured data\n",
    "import numpy as np   # For numerical operations and array management\n",
    "import pickle  # For serializing and deserializing Python objects, useful for saving models\n",
    "import os  # For interacting with the operating system, e.g., file paths\n",
    "import urllib.request # to download files\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt  # Core plotting library for static visualizations\n",
    "import seaborn as sns            # High-level interface for drawing attractive statistical graphics\n",
    "\n",
    "# --- Model Selection & Evaluation ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,  # To split the dataset into training, validation, and test sets\n",
    "    GridSearchCV,      # For exhaustive hyperparameter tuning using cross-validation\n",
    ")\n",
    "\n",
    "# --- Machine Learning Models ---\n",
    "from sklearn.ensemble import RandomForestRegressor  # Ensemble model using decision trees for regression\n",
    "import lightgbm as lgb                              # Efficient gradient boosting framework (LightGBM)\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,  # Measures average magnitude of errors in a set of predictions\n",
    "    mean_squared_error    # Penalizes larger errors more than MAE by squaring them\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889c8e6",
   "metadata": {},
   "source": [
    "## 2. Data Exploration & Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c2d9f",
   "metadata": {},
   "source": [
    "### 2.1 Load the Dataset\n",
    "Let's load the dataset we'll use for this lecture. It's the same data as in Lecture 7, so you can compare results directly.\n",
    "\n",
    "**Why?**\n",
    "A consistent dataset allows us to fairly compare linear and non-linear models.\n",
    "\n",
    "**About the Dataset**\n",
    "\n",
    "The day-ahead electricity market data used in this exercise is sourced from the **[SMARD.de](https://www.smard.de/home)** platform — the official transparency portal of the German electricity market operated by the Bundesnetzagentur.\n",
    "\n",
    "SMARD provides free access to a wide range of real-time and historical data including:\n",
    "\n",
    "- Electricity generation by source,\n",
    "- Day-ahead and intraday market prices,\n",
    "- Load forecasts and actual consumption,\n",
    "- Cross-border electricity flows.\n",
    "\n",
    "You are encouraged to explore this portal for additional datasets, either for your own projects or to extend this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a70f08",
   "metadata": {},
   "source": [
    "**This cell will create an `inputs/` folder (if it doesn't already exist) and then download the `market_data_2024.csv` file from the GitHub repository into that folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create the 'inputs' folder if it doesn't exist\n",
    "os.makedirs('inputs', exist_ok=True)\n",
    "\n",
    "# 2) Define the URL to the raw CSV on GitHub and the local path\n",
    "url = 'https://raw.githubusercontent.com/nick-harder/AIES/main/lecture8/data/market_data_2024.csv'\n",
    "local_path = 'inputs/market_data_2024.csv'\n",
    "\n",
    "# 3) Download the file only if it's not already present\n",
    "if not os.path.exists(local_path):\n",
    "    print(f\"Downloading data from {url} ...\")\n",
    "    urllib.request.urlretrieve(url, local_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c703f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('inputs/market_data_2024.csv', index_col=0, parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc934b",
   "metadata": {},
   "source": [
    "### 2.2 Correlation Analysis & Matrix\n",
    "\n",
    "Before building models, it's important to understand how your features relate to each other. The **correlation matrix** shows linear relationships between variables. High correlations can indicate redundancy (multicollinearity), which can affect some models.\n",
    "\n",
    "**Goal:** Identify strong linear relationships and potential issues in your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6479b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Pearson Correlation Analysis\n",
    "# ============================\n",
    "\n",
    "# 1. Compute Pearson correlation matrix\n",
    "# This measures linear correlation between each pair of features in the dataset.\n",
    "corr_matrix = data.corr(method='pearson')\n",
    "\n",
    "# 2. Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 3. Create a heatmap\n",
    "# - annot=True: show the actual correlation values inside the cells\n",
    "# - fmt='.2f': format correlation values to 2 decimal places\n",
    "# - cmap='coolwarm': red/blue colormap where red = strong positive, blue = strong negative\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "\n",
    "# 4. Add a descriptive title\n",
    "plt.title('Feature Correlation Matrix (Pearson)', fontsize=14)\n",
    "\n",
    "# 5. Improve layout\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae06852",
   "metadata": {},
   "source": [
    "### 2.3 Exercise 1: Correlation Analysis\n",
    "\n",
    "After computing the correlation matrix:\n",
    "\n",
    "1. **Which features are most strongly correlated with the target?**\n",
    "   - Look for the highest (positive or negative) correlation values in the target column.\n",
    "\n",
    "2. **Are there any pairs of features with very high correlation (>|0.8|)?**\n",
    "   - These indicate potential multicollinearity, which may negatively affect some models (e.g., linear regression).\n",
    "\n",
    "3. **Why might high correlation between features be a problem for some models?**\n",
    "   - Think in terms of model interpretability, overfitting, and redundancy.\n",
    "\n",
    "4. **Based on your correlation analysis, choose the top 5 features that you believe are most relevant for predicting the target variable.**\n",
    "   - Consider both strong correlation with the target and low redundancy with other features.\n",
    "   - You will use these features in subsequent modeling tasks.\n",
    "\n",
    "> **Note:** Use the `.corr()` matrix or `.abs().sort_values()` on the target column to help you decide.\n",
    "\n",
    "Once you’ve made your selection, create a new reduced feature matrix with only those 5 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86786ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# EXERCISE 1: Select Top 5 Features\n",
    "# =================================\n",
    "\n",
    "# 1. Compute the absolute correlation of each feature with the target\n",
    "corr_with_target = ... # your code here\n",
    "\n",
    "# 2. Sort the features by absolute correlation\n",
    "top_features = ... # your code here\n",
    "\n",
    "# 3. Create a new DataFrame with only the selected features\n",
    "X_selected = data[top_features]\n",
    "\n",
    "# Include the target column as well\n",
    "selected_data = data[top_features + ['DA Price']]\n",
    "\n",
    "# 4. Print the selected features\n",
    "print(\"Selected Top 5 Features:\", top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee022",
   "metadata": {},
   "source": [
    "## 3. Train/Validation/Test Split\n",
    "\n",
    "To evaluate models fairly, we split our data into **training**, **validation**, and **test** sets. This helps prevent overfitting and gives us an unbiased estimate of model performance.\n",
    "\n",
    "- **Training set:** Used to fit the model.\n",
    "- **Validation set:** Used to tune hyperparameters.\n",
    "- **Test set:** Used only for final evaluation.\n",
    "\n",
    "We'll show both manual and sklearn-based splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d11fe0",
   "metadata": {},
   "source": [
    "### 3.1 Exercise 2: Manual Data Split\n",
    "\n",
    "- Split the dataset into: training (first 80%), validation (next 10%), test (final 10%).\n",
    "- Do NOT shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Perform the split\n",
    "# ----------------------------\n",
    "X_train = ...  # your code here, first 80%\n",
    "X_eval = ...   # your code here, next 10%\n",
    "X_test = ...   # your code here, last 10%\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Extract the target column\n",
    "# ----------------------------\n",
    "y_train = X_train['DA Price']\n",
    "y_eval = X_eval['DA Price']\n",
    "y_test = X_test['DA Price']\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Drop the target from feature sets\n",
    "# ----------------------------\n",
    "X_train = X_train.drop(columns=['DA Price'])\n",
    "X_eval = X_eval.drop(columns=['DA Price'])\n",
    "X_test = X_test.drop(columns=['DA Price'])\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Print shapes to verify splits\n",
    "# ----------------------------\n",
    "print(f\"Train set: {X_train.shape}, Validation set: {X_eval.shape}, Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880693e",
   "metadata": {},
   "source": [
    "### 2.2 Train-test split using `train_test_split`\n",
    "\n",
    "Now that you’ve manually split the dataset into training, validation, and test sets based on index ranges, let's explore how to perform **the exact same split logic** using a more convenient method.\n",
    "\n",
    "If your dataset does **not require time-based ordering** (i.e., it's safe to shuffle the data), `sklearn.model_selection.train_test_split` provides a concise way to randomly split it into subsets.\n",
    "\n",
    "We’ll achieve the same 80/10/10 split by:\n",
    "- First splitting off 80% of the data for training,\n",
    "- Then splitting the remaining 20% equally into validation and test sets.\n",
    "\n",
    "This approach is especially useful when working with datasets where row order doesn't carry semantic meaning, and you want to ensure randomness and reproducibility.\n",
    "\n",
    "**Note**: For time series or sequence models, avoid shuffling — use manual, time-based splits instead as we did previously.\n",
    "\n",
    "Let’s implement the shuffle-based split now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn shuffle split\n",
    "features = selected_data.drop('DA Price', axis=1)\n",
    "target = selected_data['DA Price']\n",
    "\n",
    "# Perform a train-test split\n",
    "random_state = 42  # For reproducibility\n",
    "perform_suffle = True  # Shuffle the data before splitting\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=random_state, shuffle=perform_suffle)\n",
    "# Further split the test set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = ... # your code here, 50% of the remaining data\n",
    "\n",
    "# Print shapes to verify splits\n",
    "print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1f574",
   "metadata": {},
   "source": [
    "## 4. Random Forest Implementation\n",
    "\n",
    "Random Forests are ensemble models that combine multiple decision trees to improve predictive performance and reduce overfitting. They are especially effective for capturing non-linear relationships in data.\n",
    "\n",
    "**Key Hyperparameters**\n",
    "- `n_estimators`: Number of trees in the forest\n",
    "- `max_depth`: Maximum depth of each tree\n",
    "- `random_state`: Controls reproducibility\n",
    "\n",
    "We will now train a Random Forest Regressor and evaluate its performance on the validation set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba5831",
   "metadata": {},
   "source": [
    "### 4.1 Exercise 3: Train, Evaluate, and Save a Random Forest\n",
    "\n",
    "Your task is to implement a complete training pipeline for a Random Forest model. Write your own code for each step.\n",
    "\n",
    "1. **Instantiate the model**  \n",
    "   Use: `RandomForestRegressor(n_estimators=..., max_depth=..., random_state=42)`\n",
    "\n",
    "2. **Train the model on the training data**  \n",
    "   Use: `model.fit(X_train, y_train)`\n",
    "\n",
    "3. **Predict on the validation set**  \n",
    "   Use: `model.predict(X_val)`\n",
    "\n",
    "4. **Evaluate the model**  \n",
    "   - Compute **Mean Absolute Error (MAE)**  \n",
    "     Use: `mean_absolute_error`  \n",
    "   - Compute **Root Mean Squared Error (RMSE)**  \n",
    "     Use: `mean_squared_error` and wrap it in `np.sqrt()`\n",
    "\n",
    "5. **Print the results**  \n",
    "   Display both training and validation MAE and RMSE.\n",
    "\n",
    "6. **Save the trained model to disk**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# EXERCISE 5: Random Forest Implementation\n",
    "# ========================================\n",
    "\n",
    "# Step 1: Instantiate your RandomForestRegressor here\n",
    "rf_model = RandomForestRegressor(...) # your code here\n",
    "\n",
    "# Step 2: Fit the model to the training data\n",
    "rf_model.fit(..., ...) # your code here\n",
    "\n",
    "# Step 3: Predict on the validation set\n",
    "y_pred_val_rf = ... # your code here\n",
    "\n",
    "# Step 4: Evaluate MAE and RMSE for training and validation\n",
    "mae_train_rf = ... # your code here\n",
    "rmse_train_rf = ... # your code here\n",
    "mae_val_rf = ... # your code here\n",
    "rmse_val_rf = ... # your code here\n",
    "\n",
    "# Step 5: Print your results\n",
    "print('Training MAE:', ...)\n",
    "print('Validation MAE:', ...)\n",
    "print('Training RMSE:', ...)\n",
    "print('Validation RMSE:', ...)\n",
    "\n",
    "# Step 6: Save the model to a file using pickle\n",
    "with open('outputs/rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f64c8",
   "metadata": {},
   "source": [
    "> **Optional Challenge:**\n",
    "Try different values for `n_estimators` and `max_depth`.\n",
    "How do these changes affect validation error? Can you identify overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b01cdd",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter Tuning for Random Forest (Optional)\n",
    "\n",
    "In Random Forests, model performance and generalization depend heavily on the choice of **hyperparameters**—configuration settings that control the structure and behavior of the trees before any data is seen.\n",
    "\n",
    "Let’s explore a **grid search**, which tests different combinations of hyperparameters using cross-validation and selects the best-performing model based on a scoring metric (here, **mean absolute error**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search over number of trees and max depth\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],     # Number of trees in the forest\n",
    "    'max_depth': [5, 10, None]     # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error'  # Use negative MAE for minimization\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print('Best RF Params:', grid_rf.best_params_)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "y_pred_val_best = best_rf.predict(X_val)\n",
    "print('Tuned RF MAE:', mean_absolute_error(y_val, y_pred_val_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d1e14",
   "metadata": {},
   "source": [
    "#### Hyperparameters Explained\n",
    "\n",
    "- **`n_estimators`**:  \n",
    "  The number of trees in the forest.  \n",
    "  - More trees typically reduce variance and improve performance (up to a point).  \n",
    "  - Too few trees → underfitting. Too many → increased training time.\n",
    "\n",
    "- **`max_depth`**:  \n",
    "  The maximum number of splits each tree is allowed to make (i.e., tree depth).  \n",
    "  - Low depth → high bias (underfitting).  \n",
    "  - Very high/None → high variance (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f188c5",
   "metadata": {},
   "source": [
    "## 5. LightGBM Implementation\n",
    "\n",
    "LightGBM is a highly efficient implementation of gradient boosting that builds trees sequentially, with each one correcting the errors of the previous. It is particularly well-suited for structured/tabular data.\n",
    "\n",
    "**Key Hyperparameters**\n",
    "- `learning_rate`: Step size shrinkage to prevent overfitting\n",
    "- `num_leaves`: Controls the complexity of each tree\n",
    "- `objective`: Defines the task (e.g., 'regression')\n",
    "- `metric`: Evaluation criteria (e.g., MAE 'l1', RMSE 'rmse')\n",
    "\n",
    "We will now train a LightGBM model and evaluate its performance on the validation set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3dba1a",
   "metadata": {},
   "source": [
    "### 5.1 Exercise 4: Train and Evaluate a LightGBM Model\n",
    "\n",
    "You will now build and evaluate a LightGBM regression model using your training and validation data.\n",
    "\n",
    "#### Tasks Overview\n",
    "\n",
    "1. **Prepare Datasets**  \n",
    "   Use: `lgb.Dataset(data=X_train, label=y_train)`  \n",
    "   For validation, use `reference=lgb_train` to inform LightGBM about training statistics.\n",
    "\n",
    "2. **Define Model Parameters**  \n",
    "   Create a Python dictionary with key parameters:\n",
    "   - `'objective': 'regression'`\n",
    "   - `'metric': ['l1', 'rmse']`\n",
    "   - `'learning_rate'`: e.g. `0.1`\n",
    "   - `'num_leaves'`: e.g. `31`\n",
    "   - `'verbose': -1`\n",
    "\n",
    "3. **Train the Model**  \n",
    "   Use: `lgb.train()` with:\n",
    "   - your `params`\n",
    "   - `train_set` and `valid_sets`\n",
    "   - `num_boost_round` (e.g. 100)\n",
    "   - `callbacks=[lgb.early_stopping(stopping_rounds=5)]`\n",
    "\n",
    "4. **Make Predictions**  \n",
    "   Use: `model.predict(X_val, num_iteration=model.best_iteration)`\n",
    "\n",
    "5. **Evaluate Performance**  \n",
    "   Use:\n",
    "   - `mean_absolute_error()`  \n",
    "   - `mean_squared_error()`  \n",
    "   - Wrap with `np.sqrt()` to get RMSE\n",
    "\n",
    "6. **Print Results**  \n",
    "   Show both training and validation MAE and RMSE.\n",
    "\n",
    "7. **Save the Trained Model**  \n",
    "   Use: `model.save_model('outputs/lgbm_model.txt', num_iteration=model.best_iteration)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d47fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# EXERCISE 4: LightGBM Implementation\n",
    "# ============================\n",
    "\n",
    "# Step 1: Prepare LightGBM datasets\n",
    "lgb_train = ... # your code here\n",
    "lgb_val = ... # your code here\n",
    "\n",
    "# Step 2: Define model parameters\n",
    "params = {\n",
    "    'objective': ..., # your code here\n",
    "    'metric': [...],\n",
    "    'learning_rate': ...,\n",
    "    'num_leaves': ...,\n",
    "    'verbose': ...\n",
    "}\n",
    "\n",
    "# Step 3: Train the model with early stopping\n",
    "num_round = ... # your code here\n",
    "lgb_model = lgb.train(\n",
    "    params=..., # your code here\n",
    "    train_set=...,\n",
    "    num_boost_round=...,\n",
    "    valid_sets=[..., ...],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=...)]\n",
    ")\n",
    "\n",
    "# Step 4: Predict on validation set\n",
    "y_val_pred = ... # your code here\n",
    "\n",
    "# Step 5: Evaluate performance\n",
    "mae_train_lgb = ... # your code here\n",
    "rmse_train_lgb = ...\n",
    "mae_val_lgb = ...\n",
    "rmse_val_lgb = ...\n",
    "\n",
    "# Step 6: Print results\n",
    "print('LGB Training MAE:', ...) # your code here\n",
    "print('LGB Training RMSE:', ...)\n",
    "print('LGB Validation MAE:', ...)\n",
    "print('LGB Validation RMSE:', ...)\n",
    "\n",
    "# Step 7: Save the trained model\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "lgb_model.save_model('outputs/lgbm_model.txt', num_iteration=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f2a69",
   "metadata": {},
   "source": [
    "> **Optional Challenge:**  \n",
    "Experiment with different values of `learning_rate` and `num_leaves`.  \n",
    "How does model performance change? Can you detect underfitting or overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b37e76",
   "metadata": {},
   "source": [
    "### 5.2 Understanding the Bias–Variance Tradeoff and Early Stopping\n",
    "\n",
    "Before we finish, let’s revisit an important concept from the lecture: the **bias–variance tradeoff**.\n",
    "\n",
    "This framework helps explain why models underfit or overfit—and how techniques like **early stopping** in LightGBM can help us find the right balance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a5c87",
   "metadata": {},
   "source": [
    "#### The Bias–Variance Decomposition\n",
    "\n",
    "For a given model $\\hat{f}(x)$ and target variable $y$, the expected test error can be decomposed as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{f}(x))^2] = \\underbrace{\\text{Bias}^2}_{\\text{underfitting}} + \\underbrace{\\text{Variance}}_{\\text{overfitting}} + \\underbrace{\\sigma^2}_{\\text{irreducible error}}\n",
    "$$\n",
    "\n",
    "- **Bias**: Error due to wrong assumptions (e.g. linear model for nonlinear data).\n",
    "- **Variance**: Error due to model sensitivity to training data fluctuations.\n",
    "- **Irreducible error**: Noise inherent in the data.\n",
    "\n",
    "We aim to find a model complexity that minimizes the total error—not just training error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc04d5",
   "metadata": {},
   "source": [
    "#### How Early Stopping Controls Variance\n",
    "\n",
    "Early stopping is a regularization technique that halts training once the **validation loss stops improving**. This helps prevent overfitting (i.e., high variance) by:\n",
    "\n",
    "- Monitoring model performance on a **validation set**.\n",
    "- Stopping when further training improves fit on training data but **not on validation data**.\n",
    "\n",
    "In LightGBM, we specify:\n",
    "- `early_stopping_rounds`: number of rounds with no improvement before stopping.\n",
    "- `valid_sets`: which data to monitor.\n",
    "\n",
    "This is a practical way to find a sweet spot in the bias–variance tradeoff **without manual tuning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a38e0",
   "metadata": {},
   "source": [
    "#### What Happens When the Model Overfits?\n",
    "\n",
    "Let’s explore what happens if we **deliberately overfit** our LightGBM model.\n",
    "\n",
    "> **Try This**: Change the following parameters in the cell above:\n",
    ">\n",
    "> - `learning_rate`: increase from `0.1` to `0.3`\n",
    "> - `num_leaves`: increase from `31` to `256`\n",
    "> - `num_boost_round`: increase from `100` to `1000`\n",
    "> - Remove `early_stopping` (comment out the callback line)\n",
    ">\n",
    "> Then retrain the model and compare the **training** and **validation** MAE/RMSE.\n",
    ">\n",
    "> - Do you notice a much lower training error and a higher validation error?\n",
    "> - What does this tell you about the model’s generalization?\n",
    "\n",
    "This illustrates a classic **overfitting** scenario, where the model performs well on training data but poorly on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcf4a0",
   "metadata": {},
   "source": [
    "### 5.3 Hyperparameter Tuning for LightGBM (Optional)\n",
    "\n",
    "LightGBM offers a range of hyperparameters that can significantly affect model performance. In this section, we perform a **grid search** to find a good combination of values. \n",
    "\n",
    "We focus on key parameters that control the learning rate, tree complexity, and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c63bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Hyperparameter Tuning for LightGBM\n",
    "# ==================================\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'min_data_in_leaf': [10, 20, 50]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_lgb = GridSearchCV(\n",
    "    estimator=lgb.LGBMRegressor(objective='regression', random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and corresponding score\n",
    "print('Best LightGBM Params:', grid_lgb.best_params_)\n",
    "print('Best LightGBM MAE:', -grid_lgb.best_score_)\n",
    "\n",
    "# Use the best estimator to make predictions on the validation set\n",
    "best_lgb = grid_lgb.best_estimator_\n",
    "y_pred_val_best_lgb = best_lgb.predict(X_val)\n",
    "\n",
    "# Evaluate performance\n",
    "mae = mean_absolute_error(y_val, y_pred_val_best_lgb)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_best_lgb))\n",
    "\n",
    "print('Tuned LightGBM Validation MAE:', mae)\n",
    "print('Tuned LightGBM Validation RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b730f39",
   "metadata": {},
   "source": [
    "#### Hyperparameters Explained\n",
    "\n",
    "- **`learning_rate`**:  \n",
    "  Controls how much the model adjusts per tree.  \n",
    "  - Smaller values lead to slower but more stable learning.  \n",
    "  - Larger values speed up learning but can overshoot the optimum.\n",
    "\n",
    "- **`num_leaves`**:  \n",
    "  Maximum number of leaves per tree.  \n",
    "  - Higher values increase model complexity and accuracy but may lead to overfitting.  \n",
    "  - Smaller values limit model flexibility and may underfit.\n",
    "\n",
    "- **`max_depth`**:  \n",
    "  Limits the depth of each tree.  \n",
    "  - `-1` means no limit.  \n",
    "  - Shallower trees reduce overfitting risk but may miss patterns.\n",
    "\n",
    "- **`min_data_in_leaf`**:  \n",
    "  Minimum number of data points required in a leaf.  \n",
    "  - Helps prevent overfitting by avoiding very small, specific leaves.  \n",
    "  - Larger values make the model more conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061345f",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison on the Test Set\n",
    "\n",
    "Now that both models have been saved, we will load them from disk and evaluate their performance on the **unseen test set**.\n",
    "\n",
    "We will:\n",
    "- Load the trained Random Forest and LightGBM models,\n",
    "- Generate predictions on the test set,\n",
    "- Compute MAE and RMSE for each model,\n",
    "- Compare the results to assess performance.\n",
    "\n",
    "This is a good opportunity to verify that your saved models work correctly and produce consistent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25fd287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load Saved Random Forest Model\n",
    "# ==============================\n",
    "with open('outputs/rf_model.pkl', 'rb') as f:\n",
    "    loaded_rf = pickle.load(f)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_test_rf = loaded_rf.predict(X_test)\n",
    "rf_mae = mean_absolute_error(y_test, y_pred_test_rf)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_rf))\n",
    "\n",
    "print('Random Forest Test MAE:', rf_mae)\n",
    "print('Random Forest Test RMSE:', rf_rmse)\n",
    "\n",
    "# =========================\n",
    "# Load Saved LightGBM Model\n",
    "# =========================\n",
    "loaded_gbm = lgb.Booster(model_file='outputs/lgbm_model.txt')\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_test_gbm = loaded_gbm.predict(X_test, num_iteration=loaded_gbm.best_iteration)\n",
    "gbm_mae = mean_absolute_error(y_test, y_pred_test_gbm)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_gbm))\n",
    "\n",
    "print('LightGBM Test MAE:', gbm_mae)\n",
    "print('LightGBM Test RMSE:', gbm_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041adc6a",
   "metadata": {},
   "source": [
    "### 6.1 Scatter Plot Comparison: Predicted vs. Actual\n",
    "\n",
    "These plots compare model predictions to the actual target values from the test set.\n",
    "\n",
    "- Each point represents a single prediction.\n",
    "- The red dashed line (`y = x`) shows where **perfect predictions** would lie.\n",
    "- Points **close to the line** indicate good predictions.\n",
    "- Systematic deviations from the line reveal **biases** (e.g., consistently over- or under-predicting).\n",
    "\n",
    "By comparing both Random Forest and LightGBM side by side, you can visually assess which model aligns more closely with the ground truth across the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad62470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Visual Comparison of Predictions vs. Actual\n",
    "# ===========================================\n",
    "\n",
    "# Set figure size and layout\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# --- Random Forest ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred_test_rf, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 200)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Random Forest: Predicted vs. Actual\")\n",
    "\n",
    "# --- LightGBM ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred_test_gbm, alpha=0.6, color='orange')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 200)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"LightGBM: Predicted vs. Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cb56f",
   "metadata": {},
   "source": [
    "### 6.2 Time Series Plot: Predicted vs. Actual Values Over Time\n",
    "\n",
    "This plot compares the actual target values and model predictions **over time**.\n",
    "\n",
    "- It is useful for identifying periods where the model performs well or poorly.\n",
    "- Trends, shifts, and lagging predictions become easier to observe in a time series format.\n",
    "- However, when plotting **long time ranges**, the visualization can become **visually dense** and harder to interpret.\n",
    "- In practice, it's often more informative to zoom in on **smaller time windows** (e.g., a single day or week) to inspect prediction behavior in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Time Series Comparison: Predicted vs. Actual\n",
    "# ============================================\n",
    "\n",
    "# Create an index for plotting (assumes original ordering is preserved)\n",
    "time_index = range(len(y_test))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(time_index, y_test, label='Actual', color='black', linewidth=2)\n",
    "\n",
    "# Plot Random Forest predictions\n",
    "plt.plot(time_index, y_pred_test_rf, label='Random Forest Prediction', linestyle='--')\n",
    "\n",
    "# Plot LightGBM predictions\n",
    "plt.plot(time_index, y_pred_test_gbm, label='LightGBM Prediction', linestyle='--')\n",
    "\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"Target Value\")\n",
    "plt.title(\"Test Set Predictions Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-50, 200)  # Adjust based on your target value range\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1093229",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Visualization\n",
    "\n",
    "One major advantage of tree-based models is their ability to quantify **feature importance** — that is, how much each feature contributes to the model’s predictions.\n",
    "\n",
    "Understanding feature importance helps us:\n",
    "- Interpret model behavior,\n",
    "- Identify which variables the model relies on most,\n",
    "- Potentially simplify the model by removing unimportant features.\n",
    "\n",
    "Below, we'll extract and plot the feature importances from both the **Random Forest** and **LightGBM** models to compare them side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# --- Random Forest Feature Importance ---\n",
    "rf_importances = pd.Series(loaded_rf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "# --- LightGBM Feature Importance ---\n",
    "gbm_importances = pd.Series(\n",
    "    lgb_model.feature_importance(importance_type='gain'),\n",
    "    index=feature_names\n",
    ")\n",
    "gbm_importances = (gbm_importances / gbm_importances.sum()).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest\n",
    "rf_importances.head(10).plot(kind='barh', ax=axes[0])\n",
    "axes[0].set_title(\"Random Forest: Top 5 Feature Importances\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# LightGBM\n",
    "gbm_importances.head(10).plot(kind='barh', color='orange', ax=axes[1])\n",
    "axes[1].set_title(\"LightGBM: Top 5 Feature Importances\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cc69e",
   "metadata": {},
   "source": [
    "### 7.1 Exercise 5: Analyze Feature Importances\n",
    "\n",
    "Now that you've seen the top features identified by both Random Forest and LightGBM:\n",
    "\n",
    "1. Compare the feature importances to the **correlation matrix** you analyzed earlier.\n",
    "2. Are the most important features also the most correlated with the target?\n",
    "   - If yes, why might that be the case?\n",
    "   - If not, what could explain the difference?\n",
    "3. Do both models rank the same features as important? If not, what might explain the differences?\n",
    "\n",
    "Take a few minutes to reflect on these questions. Write down your observations and hypotheses — you'll discuss them later in class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357ac23",
   "metadata": {},
   "source": [
    "## 8. Discussion & Conclusion\n",
    "\n",
    "In this notebook, we explored **non-linear regression models**—Random Forest and LightGBM—and applied them to structured energy systems data. Starting from a **correlation analysis**, we selected a subset of features and evaluated each model using a clear **train/validation/test pipeline**.\n",
    "\n",
    "You learned to:\n",
    "\n",
    "* Identify important features based on correlation and model-derived importance,\n",
    "* Implement and tune ensemble models,\n",
    "* Use early stopping and save/load trained models,\n",
    "* Visualize and interpret performance both numerically and graphically.\n",
    "\n",
    "We found that:\n",
    "\n",
    "* Both models performed well, but their feature importance rankings differed.\n",
    "* Predictions generally followed the actual values, but visualizations showed areas where each model excelled or struggled.\n",
    "* Even with just 5 input features, the models demonstrated strong predictive power.\n",
    "\n",
    "This exercise emphasized the value of combining **statistical reasoning** (via correlation analysis) with **machine learning tools** to extract insights and build reliable predictive models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6e727",
   "metadata": {},
   "source": [
    "## 9. What to Try at Home\n",
    "\n",
    "To deepen your understanding and push your analysis further, try the following experiments:\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1. Shuffle vs. No Shuffle: Does Order Matter?\n",
    "\n",
    "In our train-test split, we used `shuffle=True`, which assumes the data is **i.i.d.** (independent and identically distributed). But what if you disable shuffling?\n",
    "\n",
    "Try:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "Observe how the performance changes. Why do you think the results differ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad534a9",
   "metadata": {},
   "source": [
    "\n",
    "### 9.2. Try K-Fold Cross-Validation\n",
    "\n",
    "Instead of relying on a single train/validation split, **K-Fold Cross-Validation** splits the data into multiple folds and trains the model on different subsets, which can give more **robust and generalized performance estimates**.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Reduces dependency on a specific split.\n",
    "* Helps validate the model more reliably on small or time-sensitive datasets.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "scores = cross_val_score(rf, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\"RMSE scores:\", rmse_scores)\n",
    "print(\"Average RMSE:\", rmse_scores.mean())\n",
    "```\n",
    "\n",
    "Try it with both models. How stable are the results across folds?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c593792",
   "metadata": {},
   "source": [
    "\n",
    "### 9.3. More Features, Better Model?\n",
    "\n",
    "We trained our models using only the **top 5 features** based on correlation. But what happens if we train using **all features**? Does the performance improve significantly?\n",
    "\n",
    "Try:\n",
    "\n",
    "* Train the same models on the full dataset (`X_all = data.drop(columns='target')`).\n",
    "* Compare MAE/RMSE against the 5-feature models.\n",
    "\n",
    "Then, try plotting the gain in performance as you include features one by one (based on importance or correlation):\n",
    "\n",
    "**Sample Sketch:**\n",
    "\n",
    "```python\n",
    "# Assume features_ranked is a list of features ordered by importance\n",
    "mae_list = []\n",
    "for i in range(1, len(features_ranked) + 1):\n",
    "    X_subset = X[features_ranked[:i]]\n",
    "    rf.fit(X_subset, y)\n",
    "    y_pred = rf.predict(X_subset)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "plt.plot(range(1, len(features_ranked)+1), mae_list)\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Performance vs. Number of Features\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Observe:\n",
    "\n",
    "* At what point do you stop gaining performance?\n",
    "* Is there a plateau or even degradation?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb2be7",
   "metadata": {},
   "source": [
    "By experimenting with these variations, you’ll not only reinforce your understanding but also build intuition around **model validation**, **feature selection**, and **generalization** — essential skills for any data scientist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
