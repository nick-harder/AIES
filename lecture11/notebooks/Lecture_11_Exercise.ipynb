{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffc0e33",
   "metadata": {},
   "source": [
    "# Lecture 11 – Classification of DER Types from Load Features\n",
    "\n",
    "**Overview**  \n",
    "In this notebook, we transition from our previous unsupervised clustering work (Lecture 10) to **supervised classification** of distributed energy resource (DER) types using household load‐feature data. You will:\n",
    "\n",
    "- Merge engineered features with DER labels  \n",
    "- Partition data into **train**, **validation**, and **test** sets  \n",
    "- Train and evaluate three models: Logistic Regression, Random Forest, and LightGBM  \n",
    "- Learn to interpret **classification metrics** (accuracy, precision, recall, F1, support)  \n",
    "- Extract **feature importances** and **decision rules** for interpretability  \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this notebook, students will be able to:\n",
    "\n",
    "1. **Prepare** labeled datasets and understand the role of train/validation/test splits.  \n",
    "2. **Explain** and implement stratified sampling to preserve class distributions.  \n",
    "3. **Train** baseline (LogReg) and advanced tree-based (RF, LightGBM) classifiers.  \n",
    "4. **Compute** and **interpret** macro-F1 scores, confusion matrices, and classification reports.  \n",
    "5. **Visualize** and **extract** feature importances and decision rules.  \n",
    "6. **Reflect** on model trade-offs in performance vs. interpretability.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ea0d1",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "\n",
    "Before we begin analyzing and modeling data, we need to import the necessary libraries. These tools cover everything from data manipulation and visualization to machine learning and evaluation.\n",
    "\n",
    "* **`pandas`** and **`numpy`**: Essential libraries for data manipulation and numerical operations.\n",
    "* **`matplotlib.pyplot`**: Used for creating static visualizations of data.\n",
    "* **`scikit-learn`**: Provides utilities for preprocessing data, building models (e.g., Random Forests, Logistic Regression), splitting datasets, and evaluating model performance.\n",
    "* **`lightgbm`**: A highly efficient gradient boosting framework that is particularly well-suited for structured data and large datasets.\n",
    "* **Additional modules**: We also utilize system tools (`os`, `pickle`, `urllib`) for file handling and downloading, and `scipy.stats.kurtosis` for statistical analysis.\n",
    "\n",
    "If `lightgbm` is not already installed, you can install it using the following command:\n",
    "\n",
    "```bash\n",
    "pip install lightgbm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Standard Library for File I/O and Downloading ────────────────────────────────\n",
    "import os                                 # Interact with the operating system (paths, directories)\n",
    "import pickle                             # Serialize and deserialize Python objects\n",
    "import urllib.request                     # Download data from URLs\n",
    "\n",
    "# ─── Data Handling and Numerical Computation ─────────────────────────────────────\n",
    "import numpy as np                        # Array operations and numerical computing\n",
    "import pandas as pd                       # Data structures (DataFrame) and data analysis\n",
    "\n",
    "# ─── Data Visualization ──────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt           # Static, interactive, and animated plotting\n",
    "\n",
    "# ─── Statistical Functions ──────────────────────────────────────────────────────\n",
    "from scipy.stats import kurtosis          # Tailedness measure of a distribution\n",
    "\n",
    "# ─── Data Preprocessing and Splitting ────────────────────────────────────────────\n",
    "from sklearn.model_selection import train_test_split  # Split data into training/testing sets\n",
    "from sklearn.preprocessing import StandardScaler     # Standardize features (zero mean, unit variance)\n",
    "\n",
    "# ─── Machine Learning Models ─────────────────────────────────────────────────────\n",
    "import lightgbm as lgb                   # Gradient boosting framework for decision trees\n",
    "from sklearn.ensemble import RandomForestClassifier  # Ensemble of decision trees for classification\n",
    "from sklearn.linear_model import LogisticRegression  # Linear classifier using logistic function\n",
    "from sklearn.manifold import TSNE                    # Dimensionality reduction for visualization\n",
    "\n",
    "# ─── Model Evaluation and Interpretation ─────────────────────────────────────────\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,                   # Fraction of correctly predicted instances\n",
    "    classification_report,            # Precision, recall, f1-score for each class\n",
    "    f1_score                          # Harmonic mean of precision and recall\n",
    ")\n",
    "from sklearn.inspection import permutation_importance  # Feature importance via permutation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91face2",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "\n",
    "Load the CSV of 15-minute consumption/injection data, inspect its shape, and plot two sample meters to get a feel for raw load profiles.\n",
    "\n",
    "This dataset is part of the corpus of quarter hourly data from 1,300 residential buildings in Flanders (Belgium) made openly available for the year 2022 by Fluvius, the Flemish DSO. We will be using parts of this dataset during our lecture 10 through 12. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d5cae",
   "metadata": {},
   "source": [
    "**This cell creates an `inputs/` folder (if it doesn't already exist) and downloads two data files from the GitHub repository into that folder:**\n",
    "\n",
    "1. `sample_smart_meter_data.parquet` — the smart meter readings dataset.\n",
    "2. `sample_smart_meter_labels.csv` — the corresponding labels indicating the category of each meter.\n",
    "\n",
    "> **Note on File Formats:**\n",
    "\n",
    "* The **Parquet** file (`.parquet`) is used to store the meter data. Parquet is a columnar storage format optimized for performance and efficiency with large datasets. It supports compression, faster querying of specific columns, and schema evolution. You can treat it like a regular Pandas DataFrame once loaded. For more details, see the [Pandas Parquet documentation](https://pandas.pydata.org/docs/user_guide/io.html#parquet).\n",
    "* The **CSV** file (`.csv`) contains category labels for each meter and is small enough that CSV remains a suitable format for it. It's straightforward to load using `pandas.read_csv()`.\n",
    "\n",
    "If you're using **Google Colab**, the required backend (`pyarrow`) for reading Parquet files is pre-installed.\n",
    "\n",
    "If you're working **locally**, ensure that `pyarrow` is installed by running:\n",
    "\n",
    "```bash\n",
    "pip install pyarrow\n",
    "```\n",
    "\n",
    "This setup allows you to efficiently load both the smart meter data and the associated labels for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ff513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'inputs' folder if it doesn't exist\n",
    "os.makedirs('inputs', exist_ok=True)\n",
    "\n",
    "# Define the files to download\n",
    "files = [\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/nick-harder/AIES/main/lecture10/data/sample_smart_meter_data.parquet\",\n",
    "        \"local_path\": \"inputs/sample_smart_meter_data.parquet\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://raw.githubusercontent.com/nick-harder/AIES/main/lecture10/data/sample_smart_meter_labels.csv\",\n",
    "        \"local_path\": \"inputs/sample_smart_meter_labels.csv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Download each file if not already present\n",
    "for file in files:\n",
    "    if not os.path.exists(file[\"local_path\"]):\n",
    "        print(f\"Downloading data from {file['url']} ...\")\n",
    "        urllib.request.urlretrieve(file[\"url\"], file[\"local_path\"])\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"File {file['local_path']} already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Load\n",
    "df = pd.read_parquet(files[0][\"local_path\"]).reset_index()\n",
    "true_labels = pd.read_csv(files[1][\"local_path\"])\n",
    "\n",
    "# parse timestamps\n",
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "\n",
    "# 1.2 Overview\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "print(\"Labels shape:\", true_labels.shape)\n",
    "print(true_labels.head())\n",
    "\n",
    "# 1.3 Plot two sample meters\n",
    "sample_ids = df[\"Meter_ID\"].unique()[:2]\n",
    "plt.figure(figsize=(12,4))\n",
    "for meter in sample_ids:\n",
    "    series = df[df[\"Meter_ID\"]==meter].set_index(\"Timestamp\")[\"Consumption_kWh\"]\n",
    "    # select only April 2023 data\n",
    "    series = series[series.index.month == 4]\n",
    "    plt.plot(series.index, series.values, label=f\"Meter {meter}\")\n",
    "plt.legend()\n",
    "plt.title(\"Raw Load Profiles for Two Sample Meters\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Consumption (kWh)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the labels\n",
    "print(\"Unique labels:\", true_labels[\"Category_Label\"].unique())\n",
    "\n",
    "# Plot the distribution of labels\n",
    "label_counts = true_labels[\"Category_Label\"].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title(\"Distribution of Labels\")\n",
    "plt.xlabel(\"Category Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b51add",
   "metadata": {},
   "source": [
    "We also fix the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17348bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75820e",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### 3.1 Domain-Agnostic Feature Extraction\n",
    "\n",
    "We compute basic statistical features over each meter’s full time series and link them to physical insights:\n",
    "\n",
    "- **Mean**: average consumption (baseline usage level)  \n",
    "- **Standard deviation**: variability (load volatility)  \n",
    "- **Skewness**: asymmetry (peak vs. off-peak behavior)  \n",
    "- **Kurtosis**: peakedness (extreme events)  \n",
    "- **Autocorrelation** at lag 1 and lag 24: temporal persistence (daily repeatability) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean(x):\n",
    "    return x.mean()\n",
    "\n",
    "def compute_std(x):\n",
    "    return x.std()\n",
    "\n",
    "def compute_skewness(x):\n",
    "    return x.skew()\n",
    "\n",
    "def compute_kurtosis(x):\n",
    "    return float(kurtosis(x, fisher=True, nan_policy='omit'))\n",
    "\n",
    "def compute_autocorr(x, lag):\n",
    "    return float(x.autocorr(lag=lag))\n",
    "\n",
    "def impute_missing(x):\n",
    "    return x.ffill().bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec661c01",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2. Domain-Informed Feature Extraction\n",
    "\n",
    "These metrics capture patterns linked to DER presence (PV panels, EV charging):\n",
    "\n",
    "- **Midday Dip**: drop around 12–14 h (PV generation reduces net consumption)  \n",
    "- **Evening Ramp**: increase 18–21 h vs. 15–18 h (EV charging starts)  \n",
    "- **Night/Day Ratio**: avg. load 00–06 h ÷ 06–18 h (overnight appliances)  \n",
    "- **Weekend Load Factor**: avg. weekend ÷ weekday (occupancy patterns)  \n",
    "- **Peak-to-Average Ratio**: peak demand ÷ mean (device spikes)  \n",
    "- **Longest Period Above Mean**: continuous high-load streaks (e.g. heating)  \n",
    "- **Longest Period of Successive Increase**: sustained rises (e.g. EV charging session)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33334781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_midday_dip(x):\n",
    "    \"\"\"\n",
    "    Measures how much the consumption drops during midday hours (12:00–14:00)\n",
    "    relative to the overall average.\n",
    "\n",
    "    A pronounced dip could suggest solar PV generation reducing net load.\n",
    "    \"\"\"\n",
    "    midday = x.between_time(\"12:00\", \"14:00\").mean()\n",
    "    return midday - x.mean()\n",
    "\n",
    "\n",
    "def compute_evening_ramp(x):\n",
    "    \"\"\"\n",
    "    Measures the increase in consumption from afternoon (15:00–18:00)\n",
    "    to evening hours (18:00–21:00).\n",
    "\n",
    "    A steep evening ramp may indicate the start of EV charging or cooking activity.\n",
    "    \"\"\"\n",
    "    eve = x.between_time(\"18:00\", \"21:00\").mean()\n",
    "    aft = x.between_time(\"15:00\", \"18:00\").mean()\n",
    "    return eve - aft\n",
    "\n",
    "\n",
    "def compute_night_day_ratio(x):\n",
    "    \"\"\"\n",
    "    Computes the ratio of average night-time load (00:00–06:00)\n",
    "    to average day-time load (06:00–18:00).\n",
    "\n",
    "    This feature helps detect households with high overnight usage (e.g., EV charging).\n",
    "    \"\"\"\n",
    "    night = x.between_time(\"00:00\", \"06:00\").mean()\n",
    "    day = x.between_time(\"06:00\", \"18:00\").mean()\n",
    "    return night / day\n",
    "\n",
    "\n",
    "def compute_weekend_load_factor(x):\n",
    "    \"\"\"\n",
    "    Calculates the ratio of weekend average load to weekday average load.\n",
    "\n",
    "    Useful to detect occupancy patterns—e.g., higher weekend usage could indicate\n",
    "    a home-based lifestyle or weekend charging.\n",
    "    \"\"\"\n",
    "    wknd = x[x.index.dayofweek >= 5].mean()  # Saturday=5, Sunday=6\n",
    "    wkday = x[x.index.dayofweek < 5].mean()  # Monday–Friday\n",
    "    return wknd / wkday\n",
    "\n",
    "\n",
    "def compute_peak_to_avg(x):\n",
    "    \"\"\"\n",
    "    Ratio of peak load to average load across the entire time series.\n",
    "\n",
    "    High values indicate \"spikiness\"—typically associated with event-driven loads like EV charging.\n",
    "    \"\"\"\n",
    "    return x.max() / x.mean()\n",
    "\n",
    "\n",
    "def compute_longest_above_mean(x):\n",
    "    \"\"\"\n",
    "    Computes the longest consecutive time period during which consumption remains\n",
    "    above the mean level.\n",
    "\n",
    "    Long high-usage periods may indicate appliances with sustained load, such as HVAC or EVs.\n",
    "    \"\"\"\n",
    "    threshold = x.mean()\n",
    "    mask = x > threshold\n",
    "\n",
    "    # If no values exceed the mean, return 0\n",
    "    if not mask.any():\n",
    "        return 0\n",
    "\n",
    "    # Identify contiguous runs above the threshold\n",
    "    runs = (mask != mask.shift()).cumsum()\n",
    "    lengths = mask.groupby(runs).sum()\n",
    "    return int(lengths.max())\n",
    "\n",
    "\n",
    "def compute_longest_increase_streak(x):\n",
    "    \"\"\"\n",
    "    Finds the longest sequence of strictly increasing consumption values\n",
    "    in the time series.\n",
    "\n",
    "    Can indicate gradual ramp-up behavior, such as slow EV charging or heat pump cycles.\n",
    "    \"\"\"\n",
    "    delta = x.diff()\n",
    "    mask = delta > 0\n",
    "\n",
    "    if not mask.any():\n",
    "        return 0\n",
    "\n",
    "    runs = (mask != mask.shift()).cumsum()\n",
    "    lengths = mask.groupby(runs).sum()\n",
    "    return int(lengths.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a38fc5",
   "metadata": {},
   "source": [
    "### 3.3 Build Feature Matrix\n",
    "\n",
    "Apply both sets of functions to each meter to assemble a feature DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29131849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactored: modular feature extraction\n",
    "def extract_features(df, meter_id):\n",
    "    series = df[df[\"Meter_ID\"]==meter_id].set_index(\"Timestamp\")[\"Consumption_kWh\"]\n",
    "    series = impute_missing(series)\n",
    "    return {\n",
    "        \"Meter_ID\": meter_id,\n",
    "        \"mean\": compute_mean(series),\n",
    "        \"std\": compute_std(series),\n",
    "        \"skew\": compute_skewness(series),\n",
    "        \"kurtosis\": compute_kurtosis(series),\n",
    "        \"autocorr_lag1\": compute_autocorr(series,1),\n",
    "        \"autocorr_lag24\": compute_autocorr(series,24),\n",
    "        \"midday_dip\": compute_midday_dip(series),\n",
    "        \"evening_ramp\": compute_evening_ramp(series),\n",
    "        \"night_day_ratio\": compute_night_day_ratio(series),\n",
    "        \"weekend_load_factor\": compute_weekend_load_factor(series),\n",
    "        \"peak_to_avg\": compute_peak_to_avg(series),\n",
    "        \"longest_above_mean\": compute_longest_above_mean(series),\n",
    "        \"longest_increase_streak\": compute_longest_increase_streak(series),\n",
    "    }\n",
    "\n",
    "meters = df[\"Meter_ID\"].unique() # Get unique meter IDs\n",
    "feature_list = [extract_features(df, m) for m in meters] # Extract features for each meter\n",
    "\n",
    "feature_df = pd.DataFrame(feature_list).set_index(\"Meter_ID\") # Convert to DataFrame\n",
    "\n",
    "print(\"Feature DataFrame shape:\", feature_df.shape) # Display shape of feature DataFrame\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7497a",
   "metadata": {},
   "source": [
    "### 3.4 Feature Standardization\n",
    "\n",
    "Normalize each feature to zero mean and unit variance for fair clustering. This ensures all features contribute equally to distance calculations. This is important for clustering algorithms like K-Means that are sensitive to feature scales.\n",
    "\n",
    "> Info: If the data is not standardized, features with larger ranges can disproportionately influence the clustering results, leading to biased clusters. Imagine if one feature ranged from 0 to 1000 while another ranged from 0 to 1; the first feature would dominate the distance calculations, skewing the clusters. Below you can see an example plot of such two features before and after standardization and the resulting clusters.\n",
    "\n",
    "In the following code cell, we standardize the feature matrix using `sklearn.preprocessing.StandardScaler`. This will ensure that each feature has a mean of 0 and a standard deviation of 1, making them comparable in the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64194f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # Initialize StandardScaler\n",
    "scaled_arr = scaler.fit_transform(feature_df) # Fit and transform the feature DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_arr, index=feature_df.index, columns=feature_df.columns) # Create a new DataFrame with scaled values\n",
    "\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a40ff4",
   "metadata": {},
   "source": [
    "### 3.5 t-SNE Visualization\n",
    "\n",
    "Project the high-dimensional feature space into 2D to explore natural groupings. For this, we use t-SNE with a perplexity of 30 and 1000 iterations. This will help us visualize the structure of the data and identify potential clusters. To read more about t-SNE, you can refer to the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) or here in a [medium article](https://medium.com/data-science/t-sne-clearly-explained-d84c537f53a).\n",
    "\n",
    "This doesn't give us clusters directly, but it helps us see how the data points are distributed in a lower-dimensional space. And maybe you can already spot some clusters visually?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=random_seed)\n",
    "coords = tsne.fit_transform(scaled_df)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(coords[:,0], coords[:,1], alpha=0.7)\n",
    "plt.title(\"t-SNE Projection of Building Features\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1902e6d1",
   "metadata": {},
   "source": [
    "## 4. Splitting the Dataset\n",
    "\n",
    "Before training any models, we must partition our data into three sets:\n",
    "\n",
    "* **Training set**: for fitting model parameters\n",
    "* **Validation set**: for hyperparameter tuning and early stopping\n",
    "* **Test set**: for *final* evaluation\n",
    "\n",
    "### 4.1. Stratified Sampling Explained\n",
    "\n",
    "**Stratification** ensures each split preserves the same proportion of DER classes as the full dataset. In scikit-learn, passing `stratify=y` to `train_test_split` automatically does this.\n",
    "\n",
    "> Note: *Conceptually*, stratification groups by label then samples proportionally. This avoids, for example, a validation set with no EV examples if EV is rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fab5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Merge features with labels\n",
    "labeled_df = feature_df.join(\n",
    "    true_labels.set_index(\"Meter_ID\")[\"Category_Label\"]\n",
    ")\n",
    "X_all = scaled_df.loc[labeled_df.index]\n",
    "y_all = labeled_df[\"Category_Label\"]\n",
    "\n",
    "# 4.2 First split: train+val vs. test (70/30), stratified\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.3,\n",
    "    stratify=y_all,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# 4.3 Second split: validation set from the test set (50/50), stratified\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_test, y_test,\n",
    "    test_size=0.5,\n",
    "    stratify=y_test,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# 4.4 Check distributions\n",
    "print(\"Full set:\", y_all.value_counts(normalize=True).round(3).to_dict())\n",
    "print(\"Train set:\", y_train.value_counts(normalize=True).round(3).to_dict())\n",
    "print(\"Val set:\", y_val.value_counts(normalize=True).round(3).to_dict())\n",
    "print(\"Test set:\", y_test.value_counts(normalize=True).round(3).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83edbaca",
   "metadata": {},
   "source": [
    "## 5. Baseline Classifier – Logistic Regression\n",
    "\n",
    "We start with a **Logistic Regression** model as a simple benchmark.\n",
    "This model is interpretable and provides a baseline for comparison with more complex models.\n",
    "\n",
    "### 5.1  Train Logistic Regression Model\n",
    "We use the `LogisticRegression` class from `sklearn.linear_model`, setting `max_iter=1000` to ensure convergence. The model is trained on the training set, and we evaluate its performance on the validation set using accuracy, precision, recall, F1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Logistic Regression model\n",
    "logreg_model = LogisticRegression(\n",
    "    random_state=random_seed,\n",
    "    max_iter=1000\n",
    ")\n",
    "# Fit the model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation\n",
    "y_val_lr = logreg_model.predict(X_val)\n",
    "\n",
    "# Save the model to a file in outputs folder\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "with open('outputs/logreg_model.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25382099",
   "metadata": {},
   "source": [
    "### 5.2 Understanding the Classification Report\n",
    "\n",
    "This report provides a detailed breakdown of how well your clustering matches the ground truth. Here's how to interpret the key metrics shown in the report:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Precision**\n",
    "- **Definition**: Of all buildings predicted to belong to a class (e.g., EV_Only), how many truly belong to that class?\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "- **High precision** means few false positives.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Recall**\n",
    "- **Definition**: Of all buildings that truly belong to a class, how many were correctly predicted?\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "- **High recall** means few false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "**3. F1-Score**\n",
    "- **Definition**: The harmonic mean of precision and recall. It balances both.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "- Useful when you want a single metric that considers both types of error.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Support**\n",
    "- **Definition**: The number of true instances for each class in the dataset.\n",
    "- This gives context to the above scores—how many examples were available for each label.\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:** Review both the confusion matrix and classification report to identify which classes are being misclassified and whether errors are systematic (e.g., always mislabeling EVs as Baseline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9827f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print training results\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "\n",
    "print(\"Training Results:\")\n",
    "y_train_pred = logreg_model.predict(X_train)\n",
    "print(\"Training Accuracy:        \", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Training Macro-F1 Score:  \", f1_score(y_train, y_train_pred, average=\"macro\"))\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"Validation Accuracy:       \", accuracy_score(y_val, y_val_lr))\n",
    "print(\"Validation Macro-F1 Score: \", f1_score(y_val, y_val_lr, average=\"macro\"))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf = pd.crosstab(y_val_lr, y_val, \n",
    "                   rownames=[f\"Predicted\"], colnames=[\"True\"])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\\n\",\n",
    "      classification_report(y_val, y_val_lr, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5368f4",
   "metadata": {},
   "source": [
    "## 6. Random Forest Classification\n",
    "\n",
    "In this chapter, we move beyond linear models to **Random Forests**, an ensemble of decision trees that often yield strong classification performance out of the box. You will:\n",
    "\n",
    "1. **Instantiate** a `RandomForestClassifier`.\n",
    "2. **Fit** it on the training data.\n",
    "3. **Predict** DER labels on the validation set.\n",
    "4. **Evaluate** using accuracy, macro-F1, confusion matrix, and classification report.\n",
    "\n",
    "This exercise will give you hands-on experience with tree-based methods and how to interpret their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd719ad",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement and Evaluate a Random Forest Classifier\n",
    "\n",
    "**Goal:** From scratch, train a `RandomForestClassifier` on **`X_train`, `y_train`**, predict on **`X_val`**, and report:\n",
    "\n",
    "- Validation **Accuracy**  \n",
    "- Validation **Macro-F1 score**  \n",
    "- **Confusion matrix**  \n",
    "- **Classification report** (precision, recall, F1, support)\n",
    "\n",
    "Choose at least one hyperparameter (e.g. `n_estimators`, `max_depth`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86863a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Scaffold\n",
    "\n",
    "# TODO: 1) Instantiate the model and tune one hyperparameter (e.g., n_estimators or max_depth)\n",
    "random_forest_model = RandomForestClassifier(\n",
    "    # TODO: set n_estimators, max_depth and random_state\n",
    ")\n",
    "\n",
    "# TODO: 2) Fit on training data\n",
    "# random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: 3) Predict on validation set\n",
    "# y_val_ex = random_forest_model.predict(X_val)\n",
    "\n",
    "# TODO: 4) Save the model to a file in outputs folder\n",
    "\n",
    "# TODO: 5) Compute and print:\n",
    "#       - Accuracy\n",
    "#       - Macro-F1\n",
    "#       - Confusion matrix\n",
    "#       - Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd025e16",
   "metadata": {},
   "source": [
    "## 7: LightGBM Classification\n",
    "\n",
    "In this exercise, you will implement a **LightGBM classifier** using the `LGBMClassifier` interface from the `lightgbm` library. Unlike the earlier Random Forest model, which builds trees independently, LightGBM builds them sequentially in a gradient boosting framework. This allows the model to focus on correcting previous errors and often results in higher accuracy with fewer trees.\n",
    "\n",
    "LightGBM also supports fast training, native multiclass classification, early stopping, and efficient use of system resources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8440e",
   "metadata": {},
   "source": [
    "### Exercise 2: Train and Evaluate a LightGBM Classifier\n",
    "\n",
    "**Objective:**\n",
    "You will:\n",
    "\n",
    "1. Instantiate an `LGBMClassifier` with multiclass settings.\n",
    "2. Train it on the training data with **early stopping** using the validation set.\n",
    "3. Predict DER labels for the validation set.\n",
    "4. Evaluate performance using accuracy, macro-F1, confusion matrix, and a classification report.\n",
    "5. Save the trained model to disk.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Use `objective='multiclass'` and set `num_class` appropriately.\n",
    "* Add early stopping using `callbacks=[lgb.early_stopping(...)]`.\n",
    "* Use `eval_metric='multi_logloss'` and include the validation set in `eval_set` in the fit() method.\n",
    "* Evaluate performance using all standard metrics.\n",
    "* Save the trained model using Python's `pickle` module to the folder `outputs/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73597383",
   "metadata": {},
   "source": [
    "**LightGBM Classifier Parameters**\n",
    "\n",
    "Before we instantiate the model, let’s review the key parameters used in the `LGBMClassifier`:\n",
    "\n",
    "```python\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective=...,\n",
    "    num_class=...,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    n_estimators=100,\n",
    "    random_state=random_seed,\n",
    "    verbosity=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "```\n",
    "\n",
    "* **`objective='multiclass'`**\n",
    "  Specifies the learning task. In our case, we are classifying DER types across more than two categories (None, PV, EV), so we use `'multiclass'`.\n",
    "\n",
    "* **`num_class=...`**\n",
    "  Required when `objective='multiclass'`. This must be set to the number of unique classes in the target variable (`y_train`).\n",
    "\n",
    "* **`learning_rate=0.1`**\n",
    "  Controls how much each new tree corrects the previous one. Lower values make learning slower but often more accurate; higher values speed up training but may overfit.\n",
    "\n",
    "* **`num_leaves=31`**\n",
    "  The maximum number of leaves per tree. More leaves allow the tree to capture more complexity, but can also lead to overfitting if too large.\n",
    "\n",
    "* **`n_estimators=100`**\n",
    "  The maximum number of boosting rounds (i.e., number of trees). Combined with early stopping, this sets the upper bound on training iterations.\n",
    "\n",
    "* **`random_state=random_seed`**\n",
    "  Ensures reproducibility. Using a fixed seed allows consistent model results across runs.\n",
    "\n",
    "* **`verbosity=-1`**\n",
    "  Suppresses LightGBM’s training output unless there is an error or warning.\n",
    "\n",
    "* **`n_jobs=-1`**\n",
    "  Uses all available CPU cores to parallelize training for faster performance.\n",
    "\n",
    "These parameters are a good default setup for small to medium multiclass problems. You’ll tune these further later in your coursework or when scaling to larger datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Instantiate the LGBMClassifier\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective=...,\n",
    "    num_class=...,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    n_estimators=100,\n",
    "    random_state=random_seed,\n",
    "    verbosity=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# TODO 2: Train with early stopping\n",
    "# lgb_model.fit(...)\n",
    "\n",
    "# TODO 3: Predict on validation set\n",
    "# y_val_lgbm = ...\n",
    "\n",
    "# TODO 4: Evaluate performance\n",
    "# print(\"Validation Accuracy:\", ...)\n",
    "# print(\"Macro-F1:\", ...)\n",
    "# print(classification_report(...))\n",
    "\n",
    "# 5: Save the model\n",
    "with open('outputs/lgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ecda7",
   "metadata": {},
   "source": [
    "## 8. Final Model Comparison on Test Set\n",
    "\n",
    "After tuning on validation, we run each model on the **test set** for an unbiased comparison. We report **Accuracy** and **Macro-F1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95120c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Load the models from the outputs folder\n",
    "with open('outputs/logreg_model.pkl', 'rb') as f:\n",
    "    logreg_model = pickle.load(f)\n",
    "\n",
    "with open('outputs/random_forest_model.pkl', 'rb') as f:\n",
    "    random_forest_model = pickle.load(f)\n",
    "\n",
    "with open('outputs/lgb_model.pkl', 'rb') as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "\n",
    "# 8.2 Predictions\n",
    "y_test_lr   = logreg_model.predict(X_test)\n",
    "y_test_rf   = random_forest_model.predict(X_test)\n",
    "y_test_lgbm = lgb_model.predict(X_test)\n",
    "\n",
    "# 8.3 Compile results\n",
    "results = pd.DataFrame({\n",
    "    'Model':       ['Logistic Regression', 'Random Forest', 'LightGBM'],\n",
    "    'Accuracy':    [\n",
    "        accuracy_score(y_test, y_test_lr),\n",
    "        accuracy_score(y_test, y_test_rf),\n",
    "        accuracy_score(y_test, y_test_lgbm)\n",
    "    ],\n",
    "    'Macro-F1':    [\n",
    "        f1_score(y_test, y_test_lr,   average='macro'),\n",
    "        f1_score(y_test, y_test_rf,   average='macro'),\n",
    "        f1_score(y_test, y_test_lgbm, average='macro')\n",
    "    ]\n",
    "}).set_index('Model')\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f32cb",
   "metadata": {},
   "source": [
    "## 9: Feature Importance\n",
    "\n",
    "In this chapter, we explore how to **interpret the decisions of a machine learning model**—specifically, our trained Random Forest—by examining the **relative importance of input features**.\n",
    "\n",
    "Understanding which features drive predictions helps answer questions like:\n",
    "\n",
    "* *Which load characteristics are most predictive of DER presence?*\n",
    "* *Are domain-informed features (e.g., \"midday dip\") more useful than basic statistics?*\n",
    "* *How do different importance measures reflect model behavior?*\n",
    "\n",
    "We’ll explore two techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c186f",
   "metadata": {},
   "source": [
    "### 9.1 Impurity-Based Feature Importance\n",
    "\n",
    "Random Forests compute **impurity-based importance** by summing up how much each feature reduces Gini impurity across all trees. These values are fast to compute and are available after training.\n",
    "\n",
    "> **Caveat:** Impurity-based importances can be biased toward features with more variability or more categories. Use them for quick insights but always cross-check with other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ed015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from the trained Random Forest\n",
    "importances = pd.Series(\n",
    "    random_forest_model.feature_importances_,\n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# Plot the top 10 features\n",
    "plt.figure(figsize=(10,5))\n",
    "importances.head(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 Features by Impurity-Based Importance (Random Forest)\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56017f97",
   "metadata": {},
   "source": [
    "### 9.2 Permutation-Based Feature Importance\n",
    "\n",
    "Permutation importance measures how **prediction performance changes** when a feature's values are randomly shuffled. It reflects how much the model actually relies on each feature to make accurate predictions.\n",
    "\n",
    "> **Advantage:** Model-agnostic and unbiased\n",
    "\n",
    "> **Disadvantage**: Slower - requires re-evaluating the model multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea42686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importances using validation set\n",
    "perm = permutation_importance(\n",
    "    random_forest_model, X_val, y_val,\n",
    "    n_repeats=10,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# Build importance series\n",
    "perm_importances = pd.Series(\n",
    "    perm.importances_mean,\n",
    "    index=X_val.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# Plot top 10 permutation importances\n",
    "plt.figure(figsize=(10,5))\n",
    "perm_importances.head(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 Features by Permutation Importance (Validation Set)\")\n",
    "plt.ylabel(\"Importance (ΔMacro-F1)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168f40d",
   "metadata": {},
   "source": [
    "### 9.3 Compare and Reflect\n",
    "\n",
    "* Do the top features differ between the two methods?\n",
    "* Are any **domain-informed features** (e.g., `midday_dip`, `evening_ramp`) ranked highly?\n",
    "* Could you remove the least important features to simplify the model?\n",
    "\n",
    "This kind of analysis can help prioritize sensor measurements, improve feature selection, and explain model behavior to stakeholders or regulators.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722780a3",
   "metadata": {},
   "source": [
    "## 10. Interpreting the Best Decision Tree\n",
    "\n",
    "This chapter focuses on **interpreting the internal logic of the most accurate individual decision tree** in the trained Random Forest. While Random Forests are powerful ensemble models, they can be difficult to explain directly due to the large number of trees involved. However, a single well-performing tree can reveal valuable, human-readable rules that describe how the model distinguishes DER types based on load features.\n",
    "\n",
    "We will:\n",
    "\n",
    "* Identify the most accurate tree in the forest using validation accuracy.\n",
    "* Visualize its top-level structure.\n",
    "* Extract and print all its decision rules in human-readable form.\n",
    "* Analyze the conditions and thresholds used to make predictions.\n",
    "\n",
    "These rules allow us to explain the model's behavior and build intuition about which features influence classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import _tree, plot_tree # Visualize decision trees\n",
    "\n",
    "# 1. Identify the best tree\n",
    "X_val_np = X_val.to_numpy()  # Convert once before the loop\n",
    "\n",
    "tree_accuracies = []\n",
    "for idx, tree in enumerate(random_forest_model.estimators_):\n",
    "    y_pred = tree.predict(X_val_np)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    y_pred_labels = random_forest_model.classes_[y_pred]\n",
    "    acc = accuracy_score(y_val, y_pred_labels)\n",
    "    tree_accuracies.append((idx, acc))\n",
    "\n",
    "best_idx, best_acc = max(tree_accuracies, key=lambda x: x[1])\n",
    "best_tree = random_forest_model.estimators_[best_idx]\n",
    "print(f\"Best Tree Index: {best_idx}, Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "\n",
    "# 2. Visualize top 3 levels\n",
    "plt.figure(figsize=(20, 8))\n",
    "plot_tree(\n",
    "    best_tree,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=random_forest_model.classes_,\n",
    "    filled=True,\n",
    "    max_depth=3,\n",
    "    fontsize=9\n",
    ")\n",
    "plt.title(\"Top Levels of the Most Accurate Decision Tree\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Rule extraction\n",
    "def extract_rules(tree, feature_names, class_names):\n",
    "    rules = []\n",
    "    def recurse(node, conditions):\n",
    "        if tree.tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_names[tree.tree_.feature[node]]\n",
    "            threshold = tree.tree_.threshold[node]\n",
    "            recurse(tree.tree_.children_left[node], conditions + [f\"({name} ≤ {threshold:.2f})\"])\n",
    "            recurse(tree.tree_.children_right[node], conditions + [f\"({name} > {threshold:.2f})\"])\n",
    "        else:\n",
    "            class_id = tree.tree_.value[node][0].argmax()\n",
    "            rules.append((\" AND \".join(conditions), class_names[class_id]))\n",
    "    recurse(0, [])\n",
    "    return rules\n",
    "\n",
    "rules = extract_rules(best_tree, X_train.columns.tolist(), random_forest_model.classes_)\n",
    "\n",
    "# 4. Print example rules\n",
    "for i, (conds, pred) in enumerate(rules[:8]):\n",
    "    print(f\"Rule {i+1}: IF {conds} THEN predict '{pred}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e45fc6",
   "metadata": {},
   "source": [
    "> Choose one rule and explain in your own words:\n",
    ">\n",
    "> * What pattern of load behavior is this rule capturing?\n",
    "> * How does it relate to PV or EV usage?\n",
    "> * Is the rule simple enough to be trusted by a domain expert?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e508cb2",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions\n",
    "\n",
    "This notebook guided you through a full **supervised classification pipeline** using household smart meter features to detect the presence and type of distributed energy resources (DERs), such as photovoltaic systems (PV) or electric vehicles (EV).\n",
    "\n",
    "Building on the unsupervised clustering work from Lecture 10, you learned how to train, evaluate, and interpret classifiers that operate on extracted load features. Throughout, the focus remained on building interpretable models and understanding the real-world meaning of model predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Concepts and Skills Covered\n",
    "\n",
    "1. **Labeled Data Preparation**\n",
    "\n",
    "   * Merged feature matrix with DER labels (None, PV, EV).\n",
    "   * Performed stratified splitting into training, validation, and test sets.\n",
    "\n",
    "2. **Baseline and Tree-Based Models**\n",
    "\n",
    "   * Trained a Logistic Regression model as a performance baseline.\n",
    "   * Implemented and evaluated Random Forest and LightGBM classifiers.\n",
    "   * Used a validation set for early stopping and hyperparameter tuning.\n",
    "\n",
    "3. **Performance Evaluation**\n",
    "\n",
    "   * Compared models using:\n",
    "\n",
    "     * Accuracy\n",
    "     * Macro-averaged F1 score\n",
    "     * Confusion matrices\n",
    "     * Classification reports (precision, recall, support)\n",
    "   * Interpreted what these metrics mean in the context of class imbalance.\n",
    "\n",
    "4. **Model Interpretability**\n",
    "\n",
    "   * Visualized and ranked feature importances using impurity-based and permutation methods.\n",
    "   * Identified the most accurate decision tree from the Random Forest.\n",
    "   * Extracted and interpreted human-readable decision rules.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### What You Have Learned\n",
    "\n",
    "* How to construct a supervised classification pipeline for DER detection.\n",
    "* Why and how to partition data into training, validation, and test sets with stratification.\n",
    "* How to use multiple evaluation metrics to assess model quality, particularly under imbalance.\n",
    "* How to extract interpretable logic from complex models.\n",
    "* How feature importance analysis can inform both trust and feature design.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To deepen your understanding and prepare for more advanced modeling tasks, consider the following directions:\n",
    "\n",
    "1. **Hyperparameter Tuning**\n",
    "\n",
    "   * Use `GridSearchCV` or `RandomizedSearchCV` to optimize model parameters.\n",
    "   * Explore effects of tuning tree depth, learning rates, and class weights.\n",
    "\n",
    "2. **Model Robustness and Fairness**\n",
    "\n",
    "   * Analyze performance variability across time (e.g., seasonal effects).\n",
    "   * Investigate fairness metrics—e.g., are DERs in rural vs. urban areas classified equally?\n",
    "\n",
    "3. **Explainability**\n",
    "\n",
    "   * Apply model-agnostic methods such as SHAP or LIME for instance-level explanations.\n",
    "   * Compare global (e.g., feature importance) and local explanations (e.g., decision paths).\n",
    "\n",
    "4. **Additional Feature Design**\n",
    "\n",
    "   * Engineer temporal features, injection signatures, appliance-specific events.\n",
    "   * Incorporate contextual data (e.g., weather, tariffs, building metadata).\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a foundation for **interpretable, high-impact machine learning in energy systems**, enabling intelligent grid planning, DER monitoring, and consumer insight generation from smart meter data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
